{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### <a id='toc1_1_1_'></a>[Reranking](#toc0_)\n",
    "\n",
    "After the core Retrieval part, one can add a ReRanking module to a RAG pipeline. While optional, this component is widely used in standard applications. The idea behind Reranking is to reorder/filter the set of chunks provided by the retriever to refine the context even more. Such process can be accomplished by various methods, some of which we will explore in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [Setup](#toc2_)    \n",
    "- [Simple Strategies](#toc3_)    \n",
    "  - [Filter](#toc3_1_)    \n",
    "  - [BM 25 reranking](#toc3_2_)    \n",
    "- [Reranking models and chains](#toc4_)    \n",
    "  - [BGE](#toc4_1_)    \n",
    "  - [ColBert](#toc4_2_)    \n",
    "- [To go the extra mile - LLM assisted reranking](#toc5_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc2_'></a>[Setup](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "os.chdir(Path.cwd().joinpath(\"..\"))\n",
    "print(Path.cwd())\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import CrossEncoderReranker\n",
    "from langchain.schema import Document\n",
    "from langchain_community.cross_encoders import HuggingFaceCrossEncoder\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from ragatouille import RAGPretrainedModel\n",
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "from lib.models import embeddings, llm\n",
    "from lib.prompts import RERANKER_AGENT_PROMPT\n",
    "from lib.utils import (\n",
    "    build_vector_store,\n",
    "    load_documents,\n",
    "    load_vector_store,\n",
    "    split_documents_basic,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the vector store and perform a simple retrieval of 10 chunks to be refined in our reranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_query = \"How can cloud adoption help financial institutions\"\n",
    "\n",
    "BASE_CHUNK_SIZE = 512\n",
    "\n",
    "# build vector_store\n",
    "base_documents = split_documents_basic(load_documents(\"data/amf_training\"), BASE_CHUNK_SIZE, include_linear_index=True)\n",
    "\n",
    "build_vector_store(\n",
    "    base_documents,\n",
    "    embeddings,\n",
    "    collection_name=\"amf_training\",\n",
    "    distance_function=\"cosine\",\n",
    "    erase_existing=False,\n",
    ")\n",
    "\n",
    "# Load Vector store / retriever\n",
    "chroma_vector_store = load_vector_store(embeddings, \"amf_training\")\n",
    "chroma_vector_store_retriever = chroma_vector_store.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_chunks = chroma_vector_store.similarity_search_with_score(test_query, k=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc3_'></a>[Simple Strategies](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reranking as a general process can be basically split into two parts: a scoring mecanism and a rule-based filter. At a basic level, the scoring mecanism ranks the chunks in terms of relevance and then the filter keeps only the top k chunks (with k variable)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc3_1_'></a>[Filter](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most basic form of reranking is one that uses the similarity scores from the retrieval part to filter the chunks. Indeed, one can implement a simple rule-based filter in which the chunks that were retrieved under a certain similarity threshold are removed. This minimizes the risk of feeding the LLM with context that is virtually useless. In the specific case where all retrieved chunks fall outside the accepted similarity zone, this can be used to trigger an automated response or a different pipeline and avoid hallucinations. For this example we have the relevance scores from 0 to 1, so we can set a 0.3 threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_chunks_on_scores(retrieved_chunks: list[tuple[Document, float]], threshold: float = 0.3) -> list[Document]:\n",
    "    \"\"\"Removes chunks under the threshold.\n",
    "\n",
    "    Args:\n",
    "        retrieved_chunks (list(tuple(Document, float))): list of retrieved chunks and associated relevance scores\n",
    "        threshold (float): threshold\n",
    "    \"\"\"\n",
    "    filtered_chunks = [chunk[0] for chunk in retrieved_chunks if chunk[1] < threshold]\n",
    "    if filtered_chunks == []:\n",
    "        raise ValueError(\"no chunks fit the similarity threshold\")\n",
    "    return filtered_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From now on we will look at different scoring methods that are distinct from the similarity score returned by the embeddings. Once a score is attributed, we will then rank and filter the chunks based on the score with a simple custom function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ranked_chunks(reranked_chunks: list[tuple[Document, float]], k: int = 5) -> list[Document]:\n",
    "    \"\"\"Uses reranked scores to rank and take the top k scores.\n",
    "\n",
    "    Args:\n",
    "        reranked_chunks (list[tuple): list of (Document, score) tuples from reranking\n",
    "        k (int, optional): top chunks to keep. Defaults to 5.\n",
    "\n",
    "    Returns:\n",
    "        list(Document): final chunks\n",
    "    \"\"\"\n",
    "    # Sort the chunks by score\n",
    "    extracted_chunks = sorted(reranked_chunks, key=lambda x: x[1])\n",
    "    # Take only the top k\n",
    "    extracted_chunks = extracted_chunks[-k:][::-1]\n",
    "    # Remove null scores if they exist and remove score values from list\n",
    "    extracted_chunks = [chunk[0] for chunk in extracted_chunks if chunk[1] > 0]\n",
    "\n",
    "    return extracted_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc3_2_'></a>[BM 25 reranking](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned previously, the basic premise of reranking relies on a score computation. Naturally, one can use many common NLP techniques to generate such scores. The BM 25 model that we looked at briefly in the retrieval part can be used for such purposes. Here, we will use a more sequential appraoch, and use the bm25 model ot rerank the chunks. \n",
    "\n",
    "bm25 works as a more refined TF-IDF scoring method. At a base level, it calculates the term frequency of the query terms within the documents, with the documents where the query words are the most frequent thus scoring better. However, it also incoporates many regularization techniques to avoid common noise sources. It thus matches based on words instead of semantic meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model implementation from the rank_bm25 package is very simple, and does not include tokenization. For better performance, one can perform some refined tokenization and lemmatization, but for the sake of this example we will stay simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_corpus = [chunk.page_content.split(\" \") for chunk in filter_chunks_on_scores(retrieved_chunks, 0.3)]\n",
    "\n",
    "tokenized_query = test_query.split(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now obtain the scores and filter the top 5 chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25 = BM25Okapi(tokenized_corpus)\n",
    "\n",
    "scores = bm25.get_scores(tokenized_query)\n",
    "print(scores)\n",
    "\n",
    "reranked_chunks = extract_ranked_chunks([(retrieved_chunks[i][0], scores[i]) for i in range(len(tokenized_corpus))])\n",
    "\n",
    "reranked_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc4_'></a>[Reranking models and chains](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the following methods, we will use a more streamlined approach that incoroporates LCEL chains. Indeed, while the above bm25 showcase is great for understanding how a reranker works, it is much less heavy to integrate the retrieval and reranking together in a chain, using the Document Compressor object, which can be used for reranking models.\n",
    "\n",
    "Our first step is to define the base retriever from the vectore store which we will then be able to use in a chain. Filtering by similarity score is integrated within langchain directly when searching with a threshold, the \"similarity\" here being 1-distance, so we set a 0.7 boundary to mimic the 0.3 cosine distance filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_retriever = chroma_vector_store.as_retriever(\n",
    "    search_type=\"similarity_score_threshold\",\n",
    "    search_kwargs={\"k\": 20, \"score_threshold\": 0.7},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea behind reranking models is to account for the shortcomings of the embedding/retrieval process, and thus such models can take many different forms.\n",
    "\n",
    "To understand how different models work, we will detail three different types of document retrieval models:\n",
    "\n",
    "* Embedding models like ada-v2 are single-vector no-interaction: the query and the documents are embedded seperately as one single vector, and the similarity score is computed at the end. This has the main advantage of computational efficiency, as the document embeddings can be pre-computed.\n",
    "\n",
    "* Cross encoder models like BGE or BERT are multi-vector full-interaction: here every snippet of the documents is encoded with the query and a score is outputted, which is then pooled. This provides deeper analysis of the relationships between the document and query, at the cost of computational performance.\n",
    "\n",
    "* Finally, models like ColBert are multi-vector late-interaction, and combine the best of both worlds: while the query and document are encoded seperately, they are also broken down into multiple snippets, which are then passed through an interaction matrix whose output is pooled to create a score that refelects deeper relationships, while allowing for pre-computation of document embeddings.\n",
    "\n",
    "We will look at two of these models next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc4_1_'></a>[BGE](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned, BGE is an example of a cross encoder model, which is a transformer-based architecture. This model is available through HuggingFace and integrable in langchain as a document compressor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first load our BGE model and wrap it into a reranker object, specifying the top k documents to extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bge_reranker = CrossEncoderReranker(model=HuggingFaceCrossEncoder(model_name=\"BAAI/bge-reranker-base\"), top_n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then build our combined retriever with the base and reranker, and invoke it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bge_augmented_retriever = ContextualCompressionRetriever(base_compressor=bge_reranker, base_retriever=chroma_retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bge_augmented_retriever.invoke(test_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc4_2_'></a>[ColBert](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now look at ColBert, a multi-vector late-interaction model, which can be used in a similar fashion. We need the ragatouille package where the model lives, but the implementation as a compressor is the same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load and wrap the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colbert_reranker = RAGPretrainedModel.from_pretrained(\"colbert-ir/colbertv2.0\").as_langchain_document_compressor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We build the chained retriever and invoke it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colbert_augmented_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=colbert_reranker, base_retriever=chroma_retriever\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colbert_augmented_retriever.invoke(test_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc5_'></a>[To go the extra mile - LLM assisted reranking](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will look at one more tehchnique that does not use reranking specific models, but instead an LLM agent to evaluate the rank of each chunk. The idea behind is pretty straightforward, as the whole retrieved set of documents is fed to an llm, with instructions to rank them by query relevance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, there is no method yet in langchain to implement an easy chain with any llm object. Some frameworks such as FlashRank or Cohere can be integrated as document compressors, but they use their own APIs for the llm. We wish to use our AzureOpenAI llms directly, so we create a custom chain with our own prompt in order to perform the same thing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will then create our pipeline using LCEL chains.\n",
    "\n",
    "We first create a simple method for document formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_documents_for_llm_reranking(documents: list[Document]) -> str:\n",
    "    \"\"\"Takes all documents from a list and assembles them into a single string with identifiers.\n",
    "\n",
    "    Args:\n",
    "        documents (list[Document]): list of LC documents\n",
    "\n",
    "    Returns:\n",
    "        str: mutli-line string\n",
    "    \"\"\"\n",
    "    context = \"\"\"\"\"\"\n",
    "    for i in range(len(documents)):\n",
    "        context += f\"\"\"{documents[i].page_content}\"\"\"\n",
    "        if i < len(documents):\n",
    "            context += f\"\"\"\n",
    "                Document {i} below:\n",
    "                \"\"\"\n",
    "    return context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now setup our llm reranking chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(RERANKER_AGENT_PROMPT)\n",
    "\n",
    "llm_rerank_chain = (\n",
    "    {\n",
    "        \"context\": RunnableLambda(lambda x: format_documents_for_llm_reranking(x[\"context\"])),\n",
    "        \"query\": RunnablePassthrough(),\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we setup the overall chain, which incorporates the previous chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_agent_reranking_chain = (\n",
    "    # We retrieve the relevant chunks\n",
    "    {\n",
    "        \"context\": chroma_retriever,\n",
    "        \"query\": RunnablePassthrough(),\n",
    "    }\n",
    "    # We get the ranked indices by the llm\n",
    "    | RunnablePassthrough.assign(llm_output=llm_rerank_chain)\n",
    "    # We match them with the documents\n",
    "    | RunnableLambda(lambda x: [x[\"context\"][i] for i in [int(j) for j in x[\"llm_output\"].split(\", \")]])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now invoke the full chain to get retrieval and reranking in one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_agent_reranking_chain.invoke(test_query)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
