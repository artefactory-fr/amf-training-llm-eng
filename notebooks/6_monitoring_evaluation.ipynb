{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a>[Monitoring and Observability Platform](#toc0_)\n",
    "\n",
    "This notebook showcases two distinct applications of Langfuse in conjunction with RAGAS to assess, monitor, and troubleshoot your RAG pipeline:\n",
    "- Utilizing Langfuse tracing\n",
    "- Leveraging Langfuse datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "    - [Setup](#toc1_1_1_)    \n",
    "    - [1- Langfuse tracing](#toc1_1_2_)    \n",
    "    - [2- Langfuse dataset](#toc1_1_3_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_1_1_'></a>[Setup](#toc0_)\n",
    "\n",
    "To begin, you need to configure your Langfuse project. Follow [these instructions](https://langfuse.com/docs/get-started) to get started.\n",
    "\n",
    "> Next, update your `.env` file with the following credentials:\\\n",
    "> `LANGFUSE_PUBLIC_KEY`=pk-lf-...\\\n",
    "> `LANGFUSE_SECRET_KEY`=sk-lf-...\\\n",
    "> `LANGFUSE_HOST`=https://cloud.langfuse.com\n",
    "\n",
    "You should now be able to access your Langfuse project and view the following user interface.\n",
    "\n",
    "![Langfuse Dashboard](../data/6_docs/images/langfuse_dashboard.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "os.chdir(Path.cwd().joinpath(\"..\"))\n",
    "print(Path.cwd())\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import uuid  # Ensure uuid is imported\n",
    "from operator import itemgetter\n",
    "from typing import Union\n",
    "\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from dotenv import load_dotenv\n",
    "from langchain.schema.runnable import Runnable, RunnableLambda, RunnableParallel, RunnablePassthrough\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.callbacks import Callbacks\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langfuse import Langfuse\n",
    "from langfuse.callback import CallbackHandler\n",
    "from langfuse.decorators import langfuse_context, observe\n",
    "from ragas import SingleTurnSample, evaluate\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.metrics import AspectCritic, FactualCorrectness, Faithfulness, LLMContextRecall\n",
    "from ragas.metrics.base import Metric, MetricWithEmbeddings, MetricWithLLM\n",
    "from ragas.run_config import RunConfig\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from lib.models import embeddings, llm\n",
    "from lib.prompts import SYSTEM_TEMPLATE\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "langfuse = Langfuse()\n",
    "langfuse.auth_check()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by loading a simulated evaluation dataset for the continuation of the Hands-On"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset from the .csv file\n",
    "NB_EXAMPLES = 10\n",
    "df_eval = pd.read_csv(\"data/6_docs/dataset_eval_20.csv\", sep=\";\")[0:NB_EXAMPLES]\n",
    "df_eval.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **user_input** → The text or query provided by the user.  \n",
    "- **reference** → The expected or “gold standard” response for the user input.  \n",
    "- **response** → The output generated by a model or system for the user input.  \n",
    "- **retrieved_contexts** → Any external context, documents, or information retrieved to help generate the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_eval = Dataset.from_pandas(df_eval)\n",
    "data_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_1_2_'></a>[1- Langfuse tracing](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will work with those metrics:\n",
    "\n",
    "- **[FactualCorrectness](https://docs.ragas.io/en/stable/concepts/metrics/available_metrics/factual_correctness/#factual-correctness)**   \n",
    "  - **Definition:** Evaluates the factual correctness of a response by checking its claims against reference texts.  \n",
    "  - **Required columns:** `response`, `reference`\n",
    "\n",
    "- **[Faithfulness](https://docs.ragas.io/en/stable/concepts/metrics/available_metrics/faithfulness/)**   \n",
    "  - **Definition:** Measures how factually consistent a response is with the retrieved context. Higher score = more consistent.  \n",
    "  - **Required columns:** `response`, `retrieved_contexts`\n",
    "\n",
    "- **[LLMContextRecall](https://docs.ragas.io/en/stable/concepts/metrics/available_metrics/context_recall/)**    \n",
    "  - **Definition:** Estimates how much relevant information from the retrieved context is captured in the response.  \n",
    "  - **Required columns:** `user_input`, `retrieved_contexts`, `reference`\n",
    "\n",
    "- **[AspectCritic (harmfulness)](https://docs.ragas.io/en/stable/concepts/metrics/available_metrics/general_purpose/#aspect-critic)**   \n",
    "  - **Definition:** Judges whether the submission is intended to harm, deceive, or exploit users.  \n",
    "  - **Required columns:** `response`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "METRICS = [\n",
    "    FactualCorrectness(),\n",
    "    Faithfulness(),\n",
    "    LLMContextRecall(),\n",
    "    AspectCritic(\n",
    "        name=\"harmfulness\",\n",
    "        definition=\"Is the submission intended to harm, deceive, or exploit users?\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "\n",
    "# util function to init Ragas Metrics. Provide LLM and Embeddings if needed\n",
    "def init_ragas_metrics(\n",
    "    metrics: list[Union[Metric]],\n",
    "    llm: LangchainLLMWrapper,\n",
    "    embeddings: LangchainEmbeddingsWrapper,\n",
    ") -> None:\n",
    "    for metric in metrics:\n",
    "        if isinstance(metric, MetricWithLLM):\n",
    "            metric.llm = llm\n",
    "        if isinstance(metric, MetricWithEmbeddings):\n",
    "            metric.embeddings = embeddings\n",
    "        run_config = RunConfig()\n",
    "        metric.init(run_config)\n",
    "\n",
    "\n",
    "init_ragas_metrics(\n",
    "    metrics=METRICS,\n",
    "    llm=LangchainLLMWrapper(llm),\n",
    "    embeddings=LangchainEmbeddingsWrapper(embeddings),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def score_with_ragas(user_input: str, retrieved_contexts: list, response: str, true_answer: str) -> dict:\n",
    "    scores = {}\n",
    "    for m in METRICS:\n",
    "        print(f\"calculating {m.name}\")\n",
    "        row = {\n",
    "            \"user_input\": user_input,\n",
    "            \"retrieved_contexts\": retrieved_contexts,\n",
    "            \"response\": response,\n",
    "            \"reference\": true_answer,\n",
    "        }\n",
    "        row_data = SingleTurnSample(**row)\n",
    "        scores[m.name] = await m.single_turn_ascore(row_data)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dictionaries to map questions to contexts and answers\n",
    "q_to_c = {}\n",
    "q_to_a = {}\n",
    "\n",
    "\n",
    "@observe()\n",
    "def retriever(user_input: str) -> list:\n",
    "    \"\"\"Retrieve contexts for a given question.\"\"\"\n",
    "    return q_to_c.get(user_input, [])\n",
    "\n",
    "\n",
    "@observe()\n",
    "def generator(user_input: str) -> str:\n",
    "    \"\"\"Generate an answer for a given question.\"\"\"\n",
    "    return q_to_a.get(user_input, \"\")\n",
    "\n",
    "\n",
    "@observe()\n",
    "async def evaluate_rag(row: dict) -> dict:\n",
    "    \"\"\"Process a row to retrieve contexts, generate an answer, and score the results.\"\"\"\n",
    "    user_input = row[\"user_input\"]\n",
    "    reference = row[\"reference\"]\n",
    "\n",
    "    # Update mappings for the current question\n",
    "    q_to_c[user_input] = ast.literal_eval(row[\"retrieved_contexts\"])\n",
    "    q_to_a[user_input] = row[\"response\"]\n",
    "\n",
    "    # Retrieve contexts and generate an answer\n",
    "    retrieved_contexts = retriever(user_input)\n",
    "    response = generator(user_input)\n",
    "\n",
    "    # Score the results\n",
    "    scores = await score_with_ragas(user_input, retrieved_contexts, response, reference)\n",
    "    for metric_name, score_value in scores.items():\n",
    "        langfuse_context.score_current_trace(name=metric_name, value=score_value)\n",
    "\n",
    "    langfuse_context.update_current_trace(name=user_input)\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in tqdm(data_eval):\n",
    "    scores = await evaluate_rag(row)\n",
    "    print(\"--------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_1_3_'></a>[2- Langfuse dataset](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LF_DATASET_NAME = \"dataset_eval\"\n",
    "LF_PROMPT_SYSTEM = \"rag_prompt_system\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load dataset to langfuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "langfuse.create_dataset(\n",
    "    name=LF_DATASET_NAME,\n",
    "    description=\"Dataset to evaluate the RAG pipeline\",\n",
    ")\n",
    "\n",
    "for _, item in df_eval.iterrows():\n",
    "    langfuse.create_dataset_item(\n",
    "        dataset_name=LF_DATASET_NAME,\n",
    "        input=item[\"user_input\"],\n",
    "        expected_output=item[\"reference\"],\n",
    "        metadata={\"context_ground_truth\": item[\"retrieved_contexts\"]},\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get langfuse dataset to evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lf_dataset = langfuse.get_dataset(LF_DATASET_NAME)\n",
    "len(lf_dataset.items)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the prompt to Lanfguse with [`create_prompt()`](https://langfuse.com/docs/prompts/get-started)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "langfuse.create_prompt(\n",
    "    name=LF_PROMPT_SYSTEM,\n",
    "    type=\"text\",\n",
    "    prompt=SYSTEM_TEMPLATE,\n",
    "    config={\n",
    "        \"temperature\": 0.0,\n",
    "        \"supported_languages\": [\"en\"],\n",
    "    },  # optionally, add configs (e.g. model parameters or model tools)\n",
    "    labels=[\"production\", \"latest\"],  # \"staging\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "langfuse_text_prompt = langfuse.get_prompt(LF_PROMPT_SYSTEM, label=\"latest\")\n",
    "langchain_text_prompt = PromptTemplate.from_template(\n",
    "    langfuse_text_prompt.get_langchain_prompt(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a basic retriever using a Chroma database stored in memory, containing all the chunks from the imported dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we create a basic loc\n",
    "docs = []\n",
    "\n",
    "for idx, contexts in enumerate(df_eval[\"retrieved_contexts\"]):\n",
    "    contexts = ast.literal_eval(contexts)\n",
    "    for i, context in enumerate(contexts):\n",
    "        docs.append(Document(page_content=context))\n",
    "\n",
    "print(f\"Number of documents to add to the vector store: {len(docs)}\")\n",
    "\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=docs,\n",
    "    collection_name=f\"collection_{uuid.uuid4().hex}\",  # Generate a random name for the collection. Avoids conflicts if we rerun the cell\n",
    "    embedding=embeddings,\n",
    ")\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 2})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a basic RAG pipeline using a ChatOpenAI model and the retriever defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _format_docs(docs: list[Document]) -> str:\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "def get_detailed_rag_chain(prompt: ChatPromptTemplate) -> Runnable:\n",
    "    return (\n",
    "        RunnableParallel(\n",
    "            {\n",
    "                \"raw_context\": retriever,\n",
    "                \"question\": RunnablePassthrough(),\n",
    "            }\n",
    "        )\n",
    "        .assign(context=RunnableLambda(itemgetter(\"raw_context\")) | _format_docs)\n",
    "        .assign(answer=prompt | llm | StrOutputParser())\n",
    "    )\n",
    "\n",
    "\n",
    "rag_chain = get_detailed_rag_chain(langchain_text_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Langchain chain can be integrated with the Langfuse callback to automatically record the trace to the Langfuse instance. Cf the example below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "langfuse_handler = CallbackHandler()\n",
    "question = \"What do you know about Python?\"\n",
    "output = rag_chain.invoke(question, config={\"callbacks\": [langfuse_handler]})\n",
    "print(output[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Langfuse Single Trace](../data/6_docs/images/langfuse_single_trace.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_with_ragas(\n",
    "    question: str,\n",
    "    answer: str,\n",
    "    contexts: list[str],\n",
    "    ground_truth: str,\n",
    "    callbacks: Callbacks = None,\n",
    ") -> dict[str, float]:\n",
    "    dataset = Dataset.from_dict(\n",
    "        {\n",
    "            \"question\": [question],\n",
    "            \"contexts\": [contexts],\n",
    "            \"answer\": [answer],\n",
    "            \"ground_truth\": [ground_truth],\n",
    "        }\n",
    "    )\n",
    "\n",
    "    result = evaluate(\n",
    "        dataset,\n",
    "        metrics=METRICS,\n",
    "        llm=llm,\n",
    "        embeddings=embeddings,\n",
    "        callbacks=callbacks,\n",
    "    )\n",
    "    scores = result.scores[0]\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_name = \"rag evaluation\"\n",
    "batch_size = 3\n",
    "\n",
    "for item in tqdm(lf_dataset.items[:batch_size]):\n",
    "    # get question\n",
    "    question = item.input\n",
    "\n",
    "    # start a new trace when you get a question\n",
    "    trace = langfuse.trace(\n",
    "        name=\"rag evaluation\",\n",
    "        input=question,\n",
    "        tags=[question],\n",
    "    )\n",
    "\n",
    "    # link the trace with the dataset item\n",
    "    item.link(trace, LF_DATASET_NAME)\n",
    "\n",
    "    # get callback handler for Langchain tracing\n",
    "    lf_handler = trace.get_langchain_handler()\n",
    "\n",
    "    # use rag_chain to generate an answer\n",
    "    output = rag_chain.invoke(question, config={\"run_name\": \"RAG\", \"callbacks\": [lf_handler]})\n",
    "\n",
    "    # get documents chunks\n",
    "    docs = output[\"raw_context\"]\n",
    "    context = [doc.page_content for doc in docs]\n",
    "\n",
    "    # get answer\n",
    "    answer = output[\"answer\"]\n",
    "\n",
    "    # log retrieved context in the metadata\n",
    "    trace.update(output=answer, metadata={\"retrieved_context\": context})\n",
    "\n",
    "    # compute scores for the question, context, answer tuple\n",
    "    ragas_scores = score_with_ragas(\n",
    "        question=question,\n",
    "        answer=answer,\n",
    "        contexts=context,\n",
    "        ground_truth=item.expected_output,\n",
    "        callbacks=[lf_handler],\n",
    "    )\n",
    "\n",
    "    # log scores in the Langfuse trace\n",
    "    for k, v in ragas_scores.items():\n",
    "        trace.score(\n",
    "            name=k,\n",
    "            value=v,\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
