{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# <a id='toc1_'></a>[Query Augmentation](#toc0_)\n",
    "\n",
    "A good way to improve the RAG's performance is to go beyond performing retrieval and generation based on the basic user query. Indeed, there are many ways to remaster the query to lead to better answer generation. In this notebook, we will look at the broad spectrum of query augmentation and study techniques that aim to enhcnace the query meaning or optimise the query itslef for better performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [Setup](#toc2_)    \n",
    "- [Enhanced Query - Conversation](#toc3_)    \n",
    "- [Optimized Query](#toc4_)    \n",
    "  - [Simple reformulation](#toc4_1_)    \n",
    "  - [Query Decomposition](#toc4_2_)    \n",
    "  - [HyDE rewriting](#toc4_3_)    \n",
    "  - [Step Back prompting](#toc4_4_)    \n",
    "- [Combination](#toc5_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc2_'></a>[Setup](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "os.chdir(Path.cwd().joinpath(\"..\"))\n",
    "print(Path.cwd())\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.hyde.base import HypotheticalDocumentEmbedder\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.output_parsers.boolean import BooleanOutputParser\n",
    "from langchain.schema import SystemMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "\n",
    "from lib.models import embeddings, llm\n",
    "from lib.prompts import (\n",
    "    QA_DECOMPOSITION_PROMPT,\n",
    "    QA_HISTORY_REWRITING_AGENT_PROMPT,\n",
    "    QA_HISTORY_ROUTING_AGENT_PROMPT,\n",
    "    QA_REFORMULATION_PROMPT,\n",
    "    QA_STEP_BACK_PROMPT,\n",
    ")\n",
    "from lib.utils import (\n",
    "    build_vector_store,\n",
    "    drop_document_duplicates,\n",
    "    load_documents,\n",
    "    load_vector_store,\n",
    "    split_documents_basic,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We instantiate all the tools we will use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_CHUNK_SIZE = 512\n",
    "\n",
    "# build vector_store\n",
    "base_documents = split_documents_basic(load_documents(\"data/3_docs\"), BASE_CHUNK_SIZE, include_linear_index=True)\n",
    "\n",
    "build_vector_store(\n",
    "    base_documents,\n",
    "    embeddings,\n",
    "    collection_name=\"3_docs\",\n",
    "    distance_function=\"cosine\",\n",
    "    erase_existing=False,\n",
    ")\n",
    "\n",
    "# Load Vector store / retriever\n",
    "chroma_vector_store = load_vector_store(embeddings, \"3_docs\")\n",
    "chroma_vector_store_retriever = chroma_vector_store.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc3_'></a>[Enhanced Query - Conversation](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we delve into techniques that try to optimize the existing query for better retrieval, we will first look at a very common challenge in RAG applications: how to incorporate conversational capabilities into a chatbot? \n",
    "\n",
    "Here we will look over this use case, when the user query refers to the previous messages in the conversation. There are two steps to take in order to make sure that the agent can answer the question well:\n",
    "\n",
    "* Identify whether the user query is related to a previous message or not. This overlaps a lot with the routing part, as such classification is usually performed at the same time as the typical routing classification. For the sake of demonstration, we will do this separately. The best way to perform such classification is to use a LLM to judge.\n",
    "* If the question is identified as conversation-related, another LLM is fed the entire conversation history and tasked to rewrite the query in a standalone way, which can then be used to perform retrieval.\n",
    "\n",
    "Wen use the memory object to load chats into memory, and easily return the conversation history. We will instantiate our meory with a simple example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory(return_messages=True, input_key=\"input\", output_key=\"output\")\n",
    "\n",
    "memory.save_context(\n",
    "    {\"input\": \"What's the capital of France?\"},\n",
    "    {\"output\": \"The capital of France is Paris.\"},\n",
    ")\n",
    "\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first define our helper functions to format the conversation history as well as the different llm prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "routing_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(content=QA_HISTORY_ROUTING_AGENT_PROMPT),\n",
    "        MessagesPlaceholder(variable_name=\"history\"),\n",
    "        (\"human\", \"{query}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "history_rewriting_prompt = ChatPromptTemplate.from_template(QA_HISTORY_REWRITING_AGENT_PROMPT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now build the query rewriting chain as its own chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_rewriting_chain = history_rewriting_prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we build the overall augmentation chain which uses it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_augmentation_chain = (\n",
    "    {\n",
    "        \"history\": lambda _: memory.load_memory_variables({})[\"history\"],\n",
    "        \"query\": RunnablePassthrough(),\n",
    "    }\n",
    "    | RunnablePassthrough.assign(context_query=routing_prompt | llm | BooleanOutputParser())\n",
    "    | RunnableLambda(lambda x: (history_rewriting_chain.invoke(x) if x[\"context_query\"] else x[\"query\"]))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now try with two different examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is 3 times 2?\"\n",
    "history_augmentation_chain.invoke({\"query\": query})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that the original query has not been altered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is its population ?\"\n",
    "history_augmentation_chain.invoke({\"query\": query})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But in this case, the query has been rewritten to include conversation context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc4_'></a>[Optimized Query](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now look at techniques that do not rely on adding more context to the query, but instead making sure said query is as optimized as possible to get the best results. Many of these techniques use llms as agents to change the query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc4_1_'></a>[Simple reformulation](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most simple of these techniques is a simple reformulation. The idea is to make the query more interrogative and precise, as sometimes the user input can be pretty vague. One can also integrate common abbreviations in the prompt in order to make them more explicit in the query for the retrieval step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a simple rewriting prompt and chain, with a vague example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_reformulation_chain = ChatPromptTemplate.from_template(QA_REFORMULATION_PROMPT) | llm | StrOutputParser()\n",
    "\n",
    "simple_reformulation_chain.invoke(\"data FS applications\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the user query is much more digestible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc4_2_'></a>[Query Decomposition](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some cases, the query may not be easy to answer as a whole, as it can be made up of multiple semi-distinct elements. In this case, a good method to use is query decomposition. The idea is very simple: as an LLM agent to break down the query into sub-queries that can then each be used for information retrieval separetely, the results being then ensembled into the context. We will showcase a simple implementation here, as Langchain does not have an integrated tool for this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first instantiate the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decomposition_prompt = ChatPromptTemplate.from_template(QA_DECOMPOSITION_PROMPT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know build our chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _split(input_text: str) -> list[str]:\n",
    "    return [chunk.strip() for chunk in input_text.split(\".\") if chunk.strip()]\n",
    "\n",
    "\n",
    "query_decomposition_chain = decomposition_prompt | llm | StrOutputParser() | RunnableLambda(lambda x: _split(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We try various examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_decomposition_chain.invoke(\"what is the time ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_decomposition_chain.invoke(\"give me the time and weather\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that this query decomposition works well for these basic examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc4_3_'></a>[HyDE rewriting](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a different vein, an advanced and common technique used is HyDE rewriting. The idea behing HyDE is simple: generate a fake answer to the user query without a RAG call, (i.e. often with false information), and then use this answer in the retrieval process, embedding it and thus performing an answer-answer matching instead of question-answer. The assumption behind is that this matching is more precise, and thus the retrieved chunks are more relevant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a commonly used tool, HyDE benefits from Langchain implementation out-of-the-box. We will thus use the available tools, but the technique itself is easy to replicate with a well-crafted prompt. Here we will use the financial qa prompt, which out of many standard prompts works best for finance related queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyde_pipeline = HypotheticalDocumentEmbedder.from_llm(llm, embeddings, \"web_search\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hyde_pipeline.llm_chain.invoke(\"What are the primary applications of data science in the field of finance?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the generated answer above, which looks like a chunk that could be retrieved. We then use the same HypotheticalDocumentEmbedder object to then automatically generate and embed the query using the defined models, and then go forward with our retrieval pipeline. We will show this in a very basic way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_vector_store.similarity_search_by_vector(\n",
    "    hyde_pipeline.embed_query(\"What are the primary applications of data science in the field of finance?\"),\n",
    "    k=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc4_4_'></a>[Step Back prompting](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last technique we will see is Step Back prompting, which works as a mix between sub query decomposition and rewriting. The idea is to have an LLM mimic human behavior by \"pausing and reflecting\" before answering a question, looker for higher level concepts or priniciples to guide the thought process. The goal is to extract an additional \"step back\" query and perform retrieval on both it and the original query, combining the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be simply implemented using an LLM call, parsing the output. We will consider the original query and step back question as subqueries in a similar fashion to the query decomposition pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_back_pipeline = {\n",
    "    \"query\": RunnablePassthrough(),\n",
    "    \"sb_query\": ChatPromptTemplate.from_template(QA_STEP_BACK_PROMPT) | llm | StrOutputParser(),\n",
    "} | RunnableLambda(lambda x: [x[\"query\"], x[\"sb_query\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_back_pipeline.invoke(\"What are the primary applications of data science in the field of finance?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc5_'></a>[Combination](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will try to combine all the previously seen techniques to create a thorough query optimisation chain. We will first take an input query with context, rewrite it to integrate conversation history, then break it down into sub-queries, then reformulate these queries and finally perform HyDE rewriting to retrieve chunks similar to synthetic answers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will reuse the chains we have defined in the previous cells."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a complex example with memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory(return_messages=True, input_key=\"input\", output_key=\"output\")\n",
    "\n",
    "memory.save_context(\n",
    "    {\"input\": \"What does AI mean \"},\n",
    "    {\n",
    "        \"output\": \"AI stands for Artificial Intelligence. It refers to computer systems or machines designed to perform tasks that typically require human intelligence, such as visual perception, speech recognition, decision-making, and language translation.\"\n",
    "    },\n",
    ")\n",
    "\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We build our subquery chain that performs reformulation, hyde rewriting and retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_query_chain = (\n",
    "    # The next step performs simple rewriting (see 2.1)\n",
    "    simple_reformulation_chain\n",
    "    # The next steps perform HyDE query rewriting (see 2.3)\n",
    "    | HypotheticalDocumentEmbedder.from_llm(llm, embeddings, \"web_search\").llm_chain\n",
    "    # The next step is the document retrieval\n",
    "    | chroma_vector_store_retriever\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now integrate it into the full chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combination_chain = (\n",
    "    # The next step performs chat history augmentation (see 1.)\n",
    "    history_augmentation_chain\n",
    "    # The next step performs query decomposition (see 2.2)\n",
    "    | query_decomposition_chain\n",
    "    # The next step runs the subquery chain for all identified sub queries (see previous cell)\n",
    "    | RunnableLambda(lambda x: [sub_query_chain.invoke(sq) for sq in x])\n",
    "    # Finally we ensmble all the retrieved documents in a unique list\n",
    "    | RunnableLambda(lambda x: drop_document_duplicates([chunk for sq_context in x for chunk in sq_context]))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We test the full chain with a relevant query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What are its applications to FS and how do you implement it\"\n",
    "combination_chain.invoke(query)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
