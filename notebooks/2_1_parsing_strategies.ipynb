{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a>[Parsing strategies](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parsing documents is a crucial step in enhancing the performance of Large Language Models (LLMs). It transforms unstructured text into structured data, making it accessible for AI models to process and analyze. This process includes extracting text, tables, images, and metadata from various document types.\n",
    "\n",
    "In this notebook, we will explore a range of parsing techniques with varying levels of complexity. Each method has its strengths and limitations, making it more suitable for specific use cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [Setup](#toc1_1_)    \n",
    "- [1- Classical parsing](#toc1_2_)    \n",
    "  - [- PyPDFLoader](#toc1_2_1_)    \n",
    "  - [- PyMuPDFLoader](#toc1_2_2_)    \n",
    "- [2- Unstructured Methods](#toc1_3_)    \n",
    "  - [- Raw unstructured](#toc1_3_1_)    \n",
    "  - [- Unstructured + Multimodal](#toc1_3_2_)    \n",
    "- [3 - Docling](#toc1_4_)    \n",
    "  - [- Raw docling](#toc1_4_1_)    \n",
    "  - [- Docling with Langchain](#toc1_4_2_)    \n",
    "- [4- Full multimodal](#toc1_5_)    \n",
    "- [5- Llamaparse](#toc1_6_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_1_'></a>[Setup](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "os.chdir(Path.cwd().joinpath(\"..\"))\n",
    "print(Path.cwd())\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import os\n",
    "from io import BytesIO\n",
    "from pathlib import Path\n",
    "\n",
    "import nest_asyncio\n",
    "from IPython.display import HTML, Markdown, display\n",
    "from langchain_community.document_loaders import PyMuPDFLoader, PyPDFLoader\n",
    "from llama_parse import LlamaParse\n",
    "from pdf2image import convert_from_path\n",
    "from unstructured.documents.elements import Image, Table\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    "\n",
    "from lib.models import llm\n",
    "\n",
    "DATA_PATH = Path(\"data/2_docs\")\n",
    "PDF_FILE = DATA_PATH / \"embedded-images-tables.pdf\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_2_'></a>[1- Classical parsing](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_2_1_'></a>[- PyPDFLoader](#toc0_)\n",
    "- It is one of the easiest and classical ways to parse a PDF document.<br>\n",
    "- It extracts basic metadata about the PDF (source and page number), and returns one document per page. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(PDF_FILE)\n",
    "pages = loader.load_and_split()\n",
    "\n",
    "# Example of the returned Documents\n",
    "print(pages[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_2_2_'></a>[- PyMuPDFLoader](#toc0_)\n",
    "- It is recognized for its speed and efficiency in parsing PDF files.<br>\n",
    "- It extracts detailed metadata about the PDF and its pages, returning one document per page. <br>\n",
    "- PyMuPDFLoader usually gives the same results quality as PyPDFLoader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyMuPDFLoader(PDF_FILE)\n",
    "pages = loader.load_and_split()\n",
    "\n",
    "# Example of the returned Documents\n",
    "print(pages[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Limitations** \n",
    "\n",
    "While classical methods of parsing are very fast and efficient, they usually miss important informations that are contained in complex parts of PDFs like tables, images or simply more complex type of PDFs.\n",
    "\n",
    "This is why it's important to explore other parsing strategies that can overcome these limitations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_3_'></a>[2- Unstructured Methods](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many documents contain a mixture of content types, including text, tables and images.\n",
    "\n",
    "Tables in PDF are important in most of our GenAI use cases since they contain important informations. Classical parsing may break up tables, corrupting the data in retrieval.\n",
    "\n",
    "Loading PDFs with table with a classical parser may corrupt the retrieval of data and impact the performance of your GenAI solution.\n",
    "1. The table structure is lost, which negatively impacts the LLM's output quality.\n",
    "2. Bad embeddings quality thus bad chunks retrieval with RAG use cases\n",
    "3. When chunking, there is a risk of splitting the table in half, which may result in the loss of information or headers.\n",
    "\n",
    "Unstructured is a python library that enables complex PDFs parsing, it's based on OCR models like yolox.\n",
    "\n",
    "\n",
    "Unstructured `partition_pdf` segments a PDF document by using a layout model. This layout model makes it possible to extract elements from pdfs (for example: Title, Text, Header, Image, Tables, etc...). Tables in unstructured can be extracted either in text or html format or as images.\n",
    "\n",
    "\n",
    "In this section we will show how to use unstructured standalone and how to combine it with other strategies.\n",
    "\n",
    "Before using unstructured you may need to install Tesseract with `brew install tesseract` and add french language `brew install tesseract-lang`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_3_1_'></a>[- Raw unstructured](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With Unstructured, we can extract tables in either HTML format or as images. In this approach we will extract tables as html.\n",
    "\n",
    "For more info about **partition_pdf** function see: https://docs.unstructured.io/open-source/core-functionality/partitioning#partition-pdf\n",
    "\n",
    "**partition_pdf** returns a list of element with their types, content and metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_partitioned_pdf = partition_pdf(\n",
    "    filename=PDF_FILE,  # PDF path\n",
    "    strategy=\"hi_res\",  # controls the method that will be used to process the PDF\n",
    "    languages=[\"en\", \"fr\"],  # languages to use for OCR\n",
    "    hi_res_model_name=\"yolox\",  # The layout detection model used when partitioning strategy\n",
    "    infer_table_structure=\"True\",  # enable extraction of tables for PDFs\n",
    "    extract_image_block_to_payload=True,  # If True, images of the element type(s) defined in 'extract_image_block_types' will be encoded as base64 data and stored in two metadata fields\n",
    "    extract_image_block_types=[\n",
    "        \"image\",\n",
    "        \"table\",\n",
    "    ],  # Images of the element type(s) specified in this list will be saved\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for el in raw_partitioned_pdf:\n",
    "    if type(el) is Table:\n",
    "        display(Markdown(el.metadata.text_as_html))\n",
    "    elif type(el) is Image:\n",
    "        # Display the image using base64\n",
    "        img_html = f'<img src=\"data:image/png;base64,{el.metadata.image_base64}\" />'\n",
    "        display(HTML(img_html))\n",
    "    else:\n",
    "        display(Markdown(el.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Limitations**\n",
    "\n",
    "Unstructured may struggle to correctly extract very complex and large tables in PDFs. Which may require multimodal capabilities.\n",
    "\n",
    "Unstructured can extract images as byte data, but these are not directly usable by an LLM. To work with images, you would need a multimodal model capable of processing both text and images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_3_2_'></a>[- Unstructured + Multimodal](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With Unstructured, we can extract tables in either HTML format or as images.\n",
    "\n",
    "In this approach, we will extract tables as images and then process them using a multimodal model. This model will convert the images into descriptive text, which can be embedded and utilized in a LLM.\n",
    "\n",
    "By converting tables into descriptive text, we preserve the information that would otherwise be lost due to structural changes, ensuring high-quality embeddings in our RAG process.\n",
    "\n",
    "In the example below, we extracted tables as images and used a multimodal model to process them. Alternatively, the tables could be extracted in HTML format (doable with unstructured) and processed using an LLM.\n",
    "\n",
    "![Unstructured Multimodal](https://i.postimg.cc/hhRmtLc0/img-unstructured-multimodal.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing PDF with unstructured\n",
    "\n",
    "raw_partitioned_pdf = partition_pdf(\n",
    "    filename=PDF_FILE,\n",
    "    strategy=\"hi_res\",\n",
    "    languages=[\"en\", \"fr\"],\n",
    "    hi_res_model_name=\"yolox\",\n",
    "    extract_image_block_to_payload=True,\n",
    "    extract_image_block_types=[\"image\", \"table\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompts used by the multimodal to transform table as image into a descriptive text. You can modify it and optimize it to fit better your use case.\n",
    "SYSTEM_PROMPT = \"Your objective is to convert the table or the graph in the image into descriptive sentences containing all the numbers and information from the table.\"\n",
    "\n",
    "HUMAN_PROMPT = \"Describe the table or the graph in the image with a paragraph of descriptive sentences\"\n",
    "\n",
    "# Iterating over elements to process tables\n",
    "full_doc = \"\"\n",
    "for el in raw_partitioned_pdf:\n",
    "    if isinstance(el, (Table, Image)):\n",
    "        base64_image = el.metadata.image_base64\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": SYSTEM_PROMPT,\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": HUMAN_PROMPT,\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\"url\": f\"data:image/png;base64,{base64_image}\"},\n",
    "                    },\n",
    "                ],\n",
    "            },\n",
    "        ]\n",
    "        ai_message = llm.invoke(messages)\n",
    "        full_doc += ai_message.content + \"\\n\\n\"\n",
    "    else:\n",
    "        full_doc += el.text + \"\\n\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Markdown(full_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_4_'></a>[3 - Docling](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Docling]((https://github.com/docling-project/docling)) is an open-source library (by IBM) that converts complex documents (like PDFs) into clean, structured formats such as Markdown or JSON, while preserving layout, tables, and hierarchy. Compared to Unstructured, it is generally faster, more accurate at extracting structured elements (especially tables and sections), and better integrated with RAG/NLP pipelines, whereas Unstructured is more generic but less precise.\n",
    "\n",
    "See a benchmark comparison here between Unstructured, Docling and LlamaParse: [benchmark](https://procycons.com/en/blogs/pdf-data-extraction-benchmark/#:~:text=Key%20Takeaways%3A,seconds%20depending%20on%20page%20count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_4_1_'></a>[- Raw docling](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docling.datamodel.base_models import InputFormat\n",
    "from docling.datamodel.pipeline_options import PdfPipelineOptions\n",
    "from docling.document_converter import DocumentConverter, PdfFormatOption\n",
    "\n",
    "pipeline_options = PdfPipelineOptions()\n",
    "pipeline_options.generate_page_images = True\n",
    "pipeline_options.generate_picture_images = True\n",
    "\n",
    "# Create the Docling converter\n",
    "converter = DocumentConverter(  # All of the below is optional, has internal defaults.\n",
    "    format_options={InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options)},\n",
    ")\n",
    "\n",
    "# Convert the document\n",
    "result = converter.convert(PDF_FILE).document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(result.export_to_markdown(image_mode=\"embedded\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_4_2_'></a>[- Docling with Langchain](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_docling import DoclingLoader\n",
    "from langchain_docling.loader import ExportType\n",
    "\n",
    "loader = DoclingLoader(file_path=PDF_FILE, export_type=ExportType.MARKDOWN)\n",
    "pages = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(pages[0].page_content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_5_'></a>[4- Full multimodal](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this method, we convert the pages of the PDF into images and use a multimodal model to rewrite the content of those pages. This approach is particularly useful for handling complex PDFs but also has some limitations.\n",
    "\n",
    "1- **Processing All Pages**: Applying this method to every page of the PDF can be resource-intensive, both in terms of cost and time.<br>\n",
    "2- **Selective Processing**: To improve efficiency, this process can be applied only to complex pages, such as those containing tables or intricate layouts, instead of all pages. (Not implemented in this notebook, but can be done using unstructured for example to detect pages contain tables)\n",
    "\n",
    "![Full Multimodal](https://i.postimg.cc/CMXH2DT5/img-full-multimodal.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting PDF to images\n",
    "\n",
    "pdf_path = PDF_FILE\n",
    "images = convert_from_path(pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting images from PIL to bytes\n",
    "\n",
    "images_as_bytes = []\n",
    "for img in images:\n",
    "    # Convert image to bytes\n",
    "    buffered = BytesIO()\n",
    "    img.save(buffered, format=\"JPEG\")  # Save image to buffer in PPM format\n",
    "    image_bytes = buffered.getvalue()\n",
    "\n",
    "    # Encode bytes to Base64\n",
    "    image_base64 = base64.b64encode(image_bytes).decode(\"utf-8\")\n",
    "\n",
    "    images_as_bytes.append(image_base64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rewriting each image page into text\n",
    "\n",
    "# Prompts used by the multimodal to rewrite the page. You can modify and optimize them to better suit your needs.\n",
    "SYSTEM_PROMPT = (\n",
    "    \"Your objective is to rewrite the text in the image that represents a PDF page. \"\n",
    "    \"When you face a table, you should extract it in **Markdown table format**, preserving all numbers and information. \"\n",
    "    \"When you face an image or a graph, you should describe it **precisely in a paragraph**, including all visual details and information it contains. \"\n",
    "    \"For regular text, simply rewrite it clearly.\"\n",
    ")\n",
    "\n",
    "HUMAN_PROMPT = \"Here's the page as image. Perform the extraction\"\n",
    "\n",
    "# Iterating over pages to rewrite them\n",
    "full_doc = \"\"\n",
    "for img in images_as_bytes:\n",
    "    base64_image = img\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": SYSTEM_PROMPT,\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": HUMAN_PROMPT,\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\"url\": f\"data:image/png;base64,{base64_image}\"},\n",
    "                },\n",
    "            ],\n",
    "        },\n",
    "    ]\n",
    "    ai_message = llm.invoke(messages)\n",
    "    full_doc += ai_message.content + \"\\n\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(full_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Limitations**\n",
    "\n",
    "Parsing PDFs as images using a multimodal model allows for capturing complex structures and detailed information within the document. This approach is usually useful for PDFs that primarily consist of tables or have layouts like research papers with text in multiple columns, etc...\n",
    "\n",
    "However, this strategy can be resource-intensive and time-consuming, making it less efficient for some use cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_6_'></a>[5- Llamaparse](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LlamaParse is a document parsing platform built by LlamaIndex. It exists as a standalone API and also as part of the LlamaCloud platform.\n",
    "\n",
    "To generate Llama parse API key, go to https://cloud.llamaindex.ai/ sign in and create one. You get to parse 1000 pages for free per day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nest_asyncio.apply()\n",
    "\n",
    "parser = LlamaParse(api_key=os.getenv(\"LLAMA_PARSE_API_KEY\"))\n",
    "\n",
    "documents = parser.load_data(PDF_FILE)\n",
    "\n",
    "documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Limitations**\n",
    "\n",
    "1- Relying on an external API like Llamaparse for processing confidential documents poses significant risks for data privacy and security concerns, especially when dealing with confidential or sensitive documents.\n",
    "\n",
    "2- From a cost perspective, external APIs can incur ongoing expenses that scale with usage, making them potentially more expensive than in-house solutions in the long term. Llamaparse offers 1000 free pages parsing per day. And then 3$ per additional 1000 pages."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
