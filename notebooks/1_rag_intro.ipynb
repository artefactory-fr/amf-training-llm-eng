{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {
    "papermill": {
     "duration": 0.045667,
     "end_time": "2024-05-23T15:20:04.318154",
     "exception": false,
     "start_time": "2024-05-23T15:20:04.272487",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <a id='toc1_'></a>[RAG example](#toc0_)\n",
    "\n",
    "> **📚 Sources:** \n",
    "* https://python.langchain.com/docs/tutorials/rag/\n",
    "* https://python.langchain.com/v0.1/docs/use_cases/question_answering/quickstart/ (legacy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [Architecture : break down each rag components with Langchain.](#toc2_)    \n",
    "  - [Indexing](#toc2_1_)    \n",
    "    - [Load](#toc2_1_1_)    \n",
    "    - [Split](#toc2_1_2_)    \n",
    "    - [Store](#toc2_1_3_)    \n",
    "  - [Retrieval](#toc2_2_)    \n",
    "  - [Generation](#toc2_3_)    \n",
    "- [RAG Pipeline: Production-Oriented Syntax](#toc3_)    \n",
    "  - [When to use LCEL, LangGraph, or built-in functions calls](#toc3_1_)    \n",
    "  - [LCEL](#toc3_2_)    \n",
    "  - [LangGraph](#toc3_3_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {
    "papermill": {
     "duration": 0.009217,
     "end_time": "2024-05-23T15:20:04.356525",
     "exception": false,
     "start_time": "2024-05-23T15:20:04.347308",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <a id='toc2_'></a>[Architecture : break down each rag components with Langchain.](#toc0_)\n",
    "We’ll create a typical RAG application, which has two main components:\n",
    "\n",
    "**Indexing**: a pipeline for ingesting data from a source and indexing it. This usually happens offline.  \n",
    "**Retrieval and generation**: the actual RAG chain, which takes the user query at run time and retrieves the relevant data from the index, then passes that to the model.\n",
    "\n",
    "The full sequence from raw data to answer will look like:\n",
    "\n",
    "**Indexing**\n",
    "1. **Load**: First we need to load our data. We’ll use [DocumentLoaders](https://python.langchain.com/docs/concepts/document_loaders/) for this.\n",
    "2. **Split**: [Text splitters](https://python.langchain.com/docs/concepts/text_splitters/) break large Documents into smaller chunks. This is useful both for indexing data and for passing it in to a model, since large chunks are harder to search over and won’t fit in a model’s finite context window.\n",
    "3. **Store**: We need somewhere to store and index our splits, so that they can later be searched over. This is often done using a [VectorStore](https://python.langchain.com/docs/concepts/vectorstores/) and [Embeddings](https://python.langchain.com/docs/how_to/embed_text/) model.\n",
    "\n",
    "**Retrieval and generation**\n",
    "1. **Retrieve**: Given a user input, relevant splits are retrieved from storage using a [Retriever](https://python.langchain.com/docs/concepts/retrievers/).\n",
    "2. **Generate**: A [ChatModel](https://python.langchain.com/docs/concepts/chat_models/) / [LLM](https://python.langchain.com/docs/modules/model_io/llms/) produces an answer using a prompt that includes the question and the retrieved data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {
    "papermill": {
     "duration": 0.067292,
     "end_time": "2024-05-23T15:20:04.435331",
     "exception": false,
     "start_time": "2024-05-23T15:20:04.368039",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {
    "papermill": {
     "duration": 0.009077,
     "end_time": "2024-05-23T15:20:04.448808",
     "exception": false,
     "start_time": "2024-05-23T15:20:04.439731",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Load environment variables from `.env` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {
    "papermill": {
     "duration": 0.053645,
     "end_time": "2024-05-23T15:20:04.507291",
     "exception": false,
     "start_time": "2024-05-23T15:20:04.453646",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "os.chdir(Path.cwd().joinpath(\"..\"))\n",
    "print(Path.cwd())\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {
    "papermill": {
     "duration": 0.009579,
     "end_time": "2024-05-23T15:20:04.528026",
     "exception": false,
     "start_time": "2024-05-23T15:20:04.518447",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Load Python dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {
    "papermill": {
     "duration": 0.96419,
     "end_time": "2024-05-23T15:20:05.501874",
     "exception": false,
     "start_time": "2024-05-23T15:20:04.537684",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import AzureChatOpenAI, AzureOpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "from lib.config import VECTOR_STORE_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## <a id='toc2_1_'></a>[Indexing](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {
    "papermill": {
     "duration": 0.003574,
     "end_time": "2024-05-23T15:20:05.509130",
     "exception": false,
     "start_time": "2024-05-23T15:20:05.505556",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### <a id='toc2_1_1_'></a>[Load](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {
    "papermill": {
     "duration": 0.008392,
     "end_time": "2024-05-23T15:20:05.521533",
     "exception": false,
     "start_time": "2024-05-23T15:20:05.513141",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We need to first load the PDF contents. We can use [DocumentLoaders](https://python.langchain.com/docs/concepts/document_loaders/) for this, which are objects that load in data from a source and return a list of [Documents](https://api.python.langchain.com/en/latest/documents/langchain_core.documents.base.Document.html). A `Document` is an object with some `page_content` (str) and `metadata` (dict).\n",
    "\n",
    "We will load PDF using `pypdf` into array of documents, where each document contains the page content and metadata with page number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {
    "papermill": {
     "duration": 1.206599,
     "end_time": "2024-05-23T15:20:06.738810",
     "exception": false,
     "start_time": "2024-05-23T15:20:05.532211",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "loader = PyPDFDirectoryLoader(\"data/1_docs\")\n",
    "pages = loader.load()\n",
    "len(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {
    "papermill": {
     "duration": 0.014749,
     "end_time": "2024-05-23T15:20:06.756880",
     "exception": false,
     "start_time": "2024-05-23T15:20:06.742131",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pages[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {
    "papermill": {
     "duration": 0.014453,
     "end_time": "2024-05-23T15:20:06.774867",
     "exception": false,
     "start_time": "2024-05-23T15:20:06.760414",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pages[0].__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {
    "papermill": {
     "duration": 0.003091,
     "end_time": "2024-05-23T15:20:06.781339",
     "exception": false,
     "start_time": "2024-05-23T15:20:06.778248",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### <a id='toc2_1_2_'></a>[Split](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {
    "papermill": {
     "duration": 0.003037,
     "end_time": "2024-05-23T15:20:06.787477",
     "exception": false,
     "start_time": "2024-05-23T15:20:06.784440",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Our loaded document can be too long to fit in the context window of many models. Even for those models that could fit the full post in their context window, models can struggle to find information in very long inputs.\n",
    "\n",
    "To handle this we’ll split the `Document` into chunks for embedding and vector storage. This should help us retrieve only the most relevant bits of the blog post at run time.\n",
    "\n",
    "In this case we’ll split our documents into chunks of 2000 characters with 200 characters of overlap between chunks. The overlap helps mitigate the possibility of separating a statement from important context related to it. We use the [RecursiveCharacterTextSplitter](https://python.langchain.com/docs/how_to/recursive_text_splitter/), which will recursively split the document using common separators like new lines until each chunk is the appropriate size. This is the recommended text splitter for generic text use cases.\n",
    "\n",
    "We set `add_start_index=True` so that the character index at which each split Document starts within the initial Document is preserved as metadata attribute “start_index”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {
    "papermill": {
     "duration": 0.016439,
     "end_time": "2024-05-23T15:20:06.807015",
     "exception": false,
     "start_time": "2024-05-23T15:20:06.790576",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=200, add_start_index=True)\n",
    "all_splits = text_splitter.split_documents(pages)\n",
    "len(all_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {
    "papermill": {
     "duration": 0.014691,
     "end_time": "2024-05-23T15:20:06.825170",
     "exception": false,
     "start_time": "2024-05-23T15:20:06.810479",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_splits[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {
    "papermill": {
     "duration": 0.014772,
     "end_time": "2024-05-23T15:20:06.843532",
     "exception": false,
     "start_time": "2024-05-23T15:20:06.828760",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_splits[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {
    "papermill": {
     "duration": 0.003424,
     "end_time": "2024-05-23T15:20:06.850645",
     "exception": false,
     "start_time": "2024-05-23T15:20:06.847221",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### <a id='toc2_1_3_'></a>[Store](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {
    "papermill": {
     "duration": 0.003569,
     "end_time": "2024-05-23T15:20:06.857656",
     "exception": false,
     "start_time": "2024-05-23T15:20:06.854087",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Now we need to index our text chunks so that we can search over them at runtime. The most common way to do this is to embed the contents of each document split and insert these embeddings into a vector database (or vector store). When we want to search over our splits, we take a text search query, embed it, and perform some sort of “similarity” search to identify the stored splits with the most similar embeddings to our query embedding. The simplest similarity measure is cosine similarity — we measure the cosine of the angle between each pair of embeddings (which are high dimensional vectors).\n",
    "\n",
    "We can embed and store all of our document splits in a single command using the [Chroma](https://python.langchain.com/docs/integrations/vectorstores/chroma) vector store and [AzureOpenAIEmbeddings](https://python.langchain.com/docs/integrations/text_embedding/) model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {
    "papermill": {
     "duration": 0.03078,
     "end_time": "2024-05-23T15:20:06.891840",
     "exception": false,
     "start_time": "2024-05-23T15:20:06.861060",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "embeddings = AzureOpenAIEmbeddings(\n",
    "    azure_endpoint=os.getenv(\"EMBEDDINGS_AZURE_OPENAI_ENDPOINT\"),\n",
    "    openai_api_key=os.getenv(\"EMBEDDINGS_AZURE_OPENAI_API_KEY\"),\n",
    "    deployment=os.getenv(\"EMBEDDINGS_AZURE_OPENAI_DEPLOYMENT_NAME\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {
    "papermill": {
     "duration": 0.303529,
     "end_time": "2024-05-23T15:20:07.199073",
     "exception": false,
     "start_time": "2024-05-23T15:20:06.895544",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "vectorstore = Chroma(embedding_function=embeddings, persist_directory=f\"{VECTOR_STORE_PATH}/1_chroma_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {
    "papermill": {
     "duration": 7.966993,
     "end_time": "2024-05-23T15:20:15.169988",
     "exception": false,
     "start_time": "2024-05-23T15:20:07.202995",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "vectorstore.add_documents(all_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {
    "papermill": {
     "duration": 0.003587,
     "end_time": "2024-05-23T15:20:15.177774",
     "exception": false,
     "start_time": "2024-05-23T15:20:15.174187",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## <a id='toc2_2_'></a>[Retrieval](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {
    "papermill": {
     "duration": 0.003377,
     "end_time": "2024-05-23T15:20:15.184639",
     "exception": false,
     "start_time": "2024-05-23T15:20:15.181262",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Now let’s write the actual application logic. We want to create a simple application that takes a user question, searches for documents relevant to that question, passes the retrieved documents and initial question to a model, and returns an answer.\n",
    "\n",
    "First we need to define our logic for searching over documents. LangChain defines a [Retriever](https://python.langchain.com/docs/concepts/retrievers/) interface which wraps an index that can return relevant `Documents` given a string query.\n",
    "\n",
    "The most common type of `Retriever` is the [VectorStoreRetriever](https://python.langchain.com/docs/how_to/vectorstore_retriever/), which uses the similarity search capabilities of a vector store to facilitate retrieval. Any `VectorStore` can easily be turned into a `Retriever` with `VectorStore.as_retriever()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {
    "papermill": {
     "duration": 0.023972,
     "end_time": "2024-05-23T15:20:15.212538",
     "exception": false,
     "start_time": "2024-05-23T15:20:15.188566",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {
    "papermill": {
     "duration": 0.25432,
     "end_time": "2024-05-23T15:20:15.471021",
     "exception": false,
     "start_time": "2024-05-23T15:20:15.216701",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "question = \"Describe the architecture of Transformers.\"\n",
    "\n",
    "retrieved_docs = retriever.invoke(question)\n",
    "len(retrieved_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {
    "papermill": {
     "duration": 0.021603,
     "end_time": "2024-05-23T15:20:15.498772",
     "exception": false,
     "start_time": "2024-05-23T15:20:15.477169",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "retrieved_docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {
    "papermill": {
     "duration": 0.004126,
     "end_time": "2024-05-23T15:20:15.507463",
     "exception": false,
     "start_time": "2024-05-23T15:20:15.503337",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## <a id='toc2_3_'></a>[Generation](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {
    "papermill": {
     "duration": 0.003946,
     "end_time": "2024-05-23T15:20:15.515438",
     "exception": false,
     "start_time": "2024-05-23T15:20:15.511492",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Let’s put it all together into a chain that takes a question, retrieves relevant documents, constructs a prompt, passes that to a model, and parses the output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "We first define a LLM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {
    "papermill": {
     "duration": 0.034713,
     "end_time": "2024-05-23T15:20:15.553846",
     "exception": false,
     "start_time": "2024-05-23T15:20:15.519133",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm = AzureChatOpenAI(\n",
    "    azure_endpoint=os.getenv(\"LLM_AZURE_OPENAI_ENDPOINT\"),\n",
    "    openai_api_key=os.getenv(\"LLM_AZURE_OPENAI_API_KEY\"),\n",
    "    openai_api_version=os.getenv(\"LLM_AZURE_OPENAI_API_VERSION\"),\n",
    "    deployment_name=os.getenv(\"LLM_AZURE_OPENAI_DEPLOYMENT_NAME\"),\n",
    "    temperature=0.0,\n",
    "    max_tokens=1024,\n",
    "    timeout=120,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {
    "papermill": {
     "duration": 13.9938,
     "end_time": "2024-05-23T15:20:29.551734",
     "exception": false,
     "start_time": "2024-05-23T15:20:15.557934",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm.invoke(\"Who are you ?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {
    "papermill": {
     "duration": 0.00846,
     "end_time": "2024-05-23T15:20:29.570779",
     "exception": false,
     "start_time": "2024-05-23T15:20:29.562319",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Then we define the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {
    "papermill": {
     "duration": 0.030531,
     "end_time": "2024-05-23T15:20:29.608999",
     "exception": false,
     "start_time": "2024-05-23T15:20:29.578468",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "PROMPT_TEMPLATE = \"\"\"You will be given a mixed of text. Use this information to \\\n",
    "provide an answer to the user question.\n",
    "Question:\n",
    "{question}\n",
    "Context:\n",
    "{context}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "We can now run all the steps sequentially to obtain an answer to our question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Describe the architecture of Transformers in 100 words.\"\n",
    "\n",
    "retrieved_docs = retriever.invoke(question)\n",
    "docs_content = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)\n",
    "prompt_inferred = prompt.invoke({\"question\": question, \"context\": docs_content})\n",
    "answer = llm.invoke(prompt_inferred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "# <a id='toc3_'></a>[RAG Pipeline: Production-Oriented Syntax](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "Once we reach this point, we have built a **basic RAG pipeline**.  \n",
    "We will now introduce a **more production-oriented syntax**, which provides several advantages:\n",
    "\n",
    "- **Support for multiple invocation modes**: without it, logic would need to be rewritten to enable streaming of output tokens or intermediate results.  \n",
    "- **Built-in support for tracing** with LangSmith and for deployments with LangGraph Platform.  \n",
    "- **Unified interface**: allows defining and running chains consistently, with built-in support for streaming, async execution, fallback models, typing, and runtime configuration.  \n",
    "- **Automatic parallelization**: enables tasks to run in parallel, improving performance and user experience.  \n",
    "- **Composability**: makes it easy to compose and modify chains, keeping code flexible and adaptable.  \n",
    "- **(LangGraph only)**: supports persistence, human-in-the-loop workflows, and other advanced features.\n",
    "\n",
    "We will now explore **two syntaxes** for building RAG pipelines: **LCEL** and **LangGraph**.\n",
    "\n",
    "## <a id='toc3_1_'></a>[When to use LCEL, LangGraph, or built-in functions calls](#toc0_)\n",
    "\n",
    "*sources : https://python.langchain.com/docs/concepts/lcel/#should-i-use-lcel* \n",
    "\n",
    "> LCEL is an orchestration solution -- it allows LangChain to handle run-time execution of chains in an optimized way.\n",
    ">\n",
    "> While we have seen users run chains with hundreds of steps in production, we generally recommend using LCEL for simpler orchestration tasks. When the application requires complex state management, branching, cycles or multiple agents, we recommend that users take advantage of LangGraph.\n",
    ">\n",
    "> In LangGraph, users define graphs that specify the application's flow. This allows users to keep using LCEL within individual nodes when LCEL is needed, while making it easy to define complex orchestration logic that is more readable and maintainable.\n",
    ">\n",
    "> Here are some guidelines:\n",
    ">\n",
    "> - If you are making a single LLM call, you don't need LCEL; instead call the underlying chat model directly.\n",
    "> - If you have a simple chain (e.g., prompt + llm + parser, simple retrieval set up etc.), LCEL is a reasonable fit, if you're taking advantage of the LCEL benefits.\n",
    "> - If you're building a complex chain (e.g., with branching, cycles, multiple agents, etc.) use LangGraph instead. Remember that you can always use LCEL within individual nodes in LangGraph.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "## <a id='toc3_2_'></a>[LCEL](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {
    "papermill": {
     "duration": 0.004847,
     "end_time": "2024-05-23T15:20:29.620165",
     "exception": false,
     "start_time": "2024-05-23T15:20:29.615318",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We’ll use the [LCEL](https://python.langchain.com/docs/expression_language/) Runnable protocol to define the chain, allowing us to - pipe together components and functions in a transparent way - automatically trace our chain in LangSmith - get streaming, async, and batched calling out of the box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {
    "papermill": {
     "duration": 0.071995,
     "end_time": "2024-05-23T15:20:29.696594",
     "exception": false,
     "start_time": "2024-05-23T15:20:29.624599",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from typing import List  # noqa: UP035\n",
    "\n",
    "from langchain_core.runnables import Runnable, RunnableLambda, RunnableParallel\n",
    "\n",
    "\n",
    "def format_docs(docs: List[Document]) -> str:  # noqa: UP006\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "def get_rag_chain() -> Runnable:\n",
    "    return (\n",
    "        {\n",
    "            \"context\": retriever | format_docs,\n",
    "            \"question\": RunnablePassthrough(),\n",
    "        }\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "\n",
    "rag_chain = get_rag_chain()\n",
    "rag_chain.get_graph().print_ascii()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44",
   "metadata": {
    "papermill": {
     "duration": 0.02654,
     "end_time": "2024-05-23T15:20:29.729449",
     "exception": false,
     "start_time": "2024-05-23T15:20:29.702909",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Basic invocation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {
    "papermill": {
     "duration": 20.475552,
     "end_time": "2024-05-23T15:20:50.209667",
     "exception": false,
     "start_time": "2024-05-23T15:20:29.734115",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "question = \"Describe the architecture of Transformers in 500 words.\"\n",
    "\n",
    "rag_chain.invoke(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46",
   "metadata": {},
   "source": [
    "Invocation with streaming:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in rag_chain.stream(question):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48",
   "metadata": {
    "papermill": {
     "duration": 0.009302,
     "end_time": "2024-05-23T15:21:08.214028",
     "exception": false,
     "start_time": "2024-05-23T15:21:08.204726",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Get detailed output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {
    "papermill": {
     "duration": 0.031619,
     "end_time": "2024-05-23T15:21:08.253988",
     "exception": false,
     "start_time": "2024-05-23T15:21:08.222369",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_detailed_rag_chain() -> Runnable:\n",
    "    return (\n",
    "        RunnableParallel(\n",
    "            {\n",
    "                \"source_documents\": retriever,\n",
    "                \"question\": RunnablePassthrough(),\n",
    "            }\n",
    "        )\n",
    "        | RunnableParallel(\n",
    "            {\n",
    "                \"source_documents\": itemgetter(\"source_documents\"),\n",
    "                \"context\": RunnableLambda(itemgetter(\"source_documents\")) | format_docs,\n",
    "                \"question\": itemgetter(\"question\"),\n",
    "            }\n",
    "        )\n",
    "        | RunnableParallel(\n",
    "            {\n",
    "                \"source_documents\": itemgetter(\"source_documents\"),\n",
    "                \"answer\": prompt | llm | StrOutputParser(),\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "rag_chain = get_detailed_rag_chain()\n",
    "rag_chain.get_graph().print_ascii()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {
    "papermill": {
     "duration": 17.173738,
     "end_time": "2024-05-23T15:21:25.433773",
     "exception": false,
     "start_time": "2024-05-23T15:21:08.260035",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "result = rag_chain.invoke(question)\n",
    "print(result.keys())\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51",
   "metadata": {
    "papermill": {
     "duration": 0.009199,
     "end_time": "2024-05-23T15:21:25.466156",
     "exception": false,
     "start_time": "2024-05-23T15:21:25.456957",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "More consise way with .assign method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {
    "papermill": {
     "duration": 0.052661,
     "end_time": "2024-05-23T15:21:25.535206",
     "exception": false,
     "start_time": "2024-05-23T15:21:25.482545",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_detailed_rag_chain() -> Runnable:\n",
    "    return (\n",
    "        RunnableParallel(\n",
    "            {\n",
    "                \"source_documents\": retriever,\n",
    "                \"question\": RunnablePassthrough(),\n",
    "            }\n",
    "        )\n",
    "        .assign(context=RunnableLambda(itemgetter(\"source_documents\")) | format_docs)\n",
    "        .assign(answer=prompt | llm | StrOutputParser())\n",
    "    )\n",
    "\n",
    "\n",
    "rag_chain = get_detailed_rag_chain()\n",
    "rag_chain.get_graph().print_ascii()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {
    "papermill": {
     "duration": 40.381912,
     "end_time": "2024-05-23T15:22:05.924851",
     "exception": false,
     "start_time": "2024-05-23T15:21:25.542939",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "result = rag_chain.invoke(question)\n",
    "print(result.keys())\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54",
   "metadata": {},
   "source": [
    "#### <a id='toc3_2_1_1_'></a>[Drawbacks of LCEL](#toc0_)\n",
    "\n",
    "> sources: [Unleashing Th power of LCEL, from Proof of Concept to Production](https://medium.com/artefact-engineering-and-data-science/unleashing-the-power-of-langchain-expression-language-lcel-from-proof-of-concept-to-production-8ad8eebdcb1d)\n",
    "\n",
    "Despite its advantages, LCEL does have some potential drawbacks:\n",
    "\n",
    "* **Not fully PEP compliant**: LCEL does not fully respect PEP20, the Zen of Python, which states that “explicit is better than implicit”. (To check PEP20 you can run import this in python). Additionally, LCEL’s syntax is not considered “Pythonic” as it feel like a different language, this could make LCEL less intuitive for some Python developers.\n",
    "* **LCEL is a Domain-Specific Language (DSL)**: Users are expected to have some understanding of prompts, chains or LLMs in order to leverage the syntax efficiently.\n",
    "* **Input / Output dependencies**: Intermediary inputs and final outputs must be passed down from the start to the end. For instance, if you want to use the output of an intermediate step as the final output, you must carry it through all subsequent steps. This can lead to extra arguments in most of your chains, which may not be used but are necessary if you want to access them through the output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55",
   "metadata": {},
   "source": [
    "## <a id='toc3_3_'></a>[LangGraph](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56",
   "metadata": {},
   "source": [
    "LangGraph extends LangChain with a visual, graph-based interface for designing AI workflows, supporting stateful orchestration and multi-agent systems. It excels for complex, adaptive workflows, offers a more Pythonic syntax than LCEL for defining chains programmatically, but can add overhead and latency for simpler applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "from langgraph.graph import START, StateGraph\n",
    "from typing_extensions import List, TypedDict\n",
    "\n",
    "\n",
    "# 1 - Define state for application\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    context: List[Document]\n",
    "    answer: str\n",
    "\n",
    "\n",
    "# 2 - Define application steps : \"Nodes\"\n",
    "def retrieve(state: State) -> dict[str, List[Document]]:\n",
    "    retrieved_docs = retriever.invoke(state[\"question\"])\n",
    "    return {\"context\": retrieved_docs}\n",
    "\n",
    "\n",
    "def generate(state: State) -> dict[str, str]:\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
    "    messages = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})\n",
    "    response = llm.invoke(messages)\n",
    "    return {\"answer\": response.content}\n",
    "\n",
    "\n",
    "# 3 - Define edges between nodes\n",
    "graph_builder = StateGraph(State).add_sequence([retrieve, generate])\n",
    "graph_builder.add_edge(START, \"retrieve\")\n",
    "\n",
    "# 4- Compile the graph\n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = graph.invoke({\"question\": \"Describe the architecture of Transformers in 500 words.\"})\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60",
   "metadata": {},
   "source": [
    "Streaming invocation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "for message, metadata in graph.stream(\n",
    "    {\"question\": \"Describe the architecture of Transformers in 500 words.\"}, stream_mode=\"messages\"\n",
    "):\n",
    "    print(message.content, end=\"\", flush=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 123.718926,
   "end_time": "2024-05-23T15:22:06.598091",
   "environment_variables": {},
   "exception": null,
   "input_path": "notebooks/rag-example.ipynb",
   "output_path": "_ignore/run/temp/rag-example.ipynb",
   "parameters": {},
   "start_time": "2024-05-23T15:20:02.879165",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
