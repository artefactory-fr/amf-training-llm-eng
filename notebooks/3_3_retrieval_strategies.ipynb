{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a>[Retrieval](#toc0_)\n",
    "\n",
    "### <a id='toc1_1_1_'></a>[Why RAG ?](#toc0_)\n",
    "\n",
    "LLMs are usually good for answering questions based on their knowledge from training data. However, for more complex and knowledge-intensive tasks, it's possible to build a language model-based system that accesses external knowledge sources to complete tasks. This enables more factual consistency, improves reliability of the generated responses, and helps to mitigate the problem of \"hallucination\".\n",
    "\n",
    "RAG (retrieval augmented generation) takes an input and retrieves a set of relevant/supporting documents given a source (e.g., Wikipedia). The documents are concatenated as context with the original input prompt and fed to the text generator which produces the final output. This makes RAG adaptive for situations where facts could evolve over time. This is very useful as LLMs's parametric knowledge is static. RAG allows language models to bypass retraining, enabling access to the latest information for generating reliable outputs via \n",
    "retrieval-based generation.\n",
    "\n",
    "When building your Q&A chatbot, you may want to give it knowledge about your company for example. Here we want to build a chatbot able to answer questions regarding what we have already done at Artefact and give us the reference to the most relevant documents we already produced.\n",
    "\n",
    "Here's how the process looks like : \n",
    "\n",
    "![Alt Text](https://miro.medium.com/v2/resize:fit:1400/1*UyhiO87T-hejRhqI7EwvgA.png)\n",
    "\n",
    "### <a id='toc1_1_2_'></a>[Chunking](#toc0_)\n",
    "\n",
    "In this notebook, we will explore the Retrieval part of the pipeline. The retriever is the component that looks within the vector store for the chunks that correspond to the user query and then extracts such chunks, to then be fed to the LLM as context. The retriever is the core of the RAG architecture, and uses a similarity analysis to identify the embedded chunks within the vector space.\n",
    "\n",
    "Here, we will showcase a basic retrieval, and then study three other more advanced ways to extract the relevant context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [Retrieval](#toc1_)    \n",
    "    - [Why RAG ?](#toc1_1_1_)    \n",
    "    - [Chunking](#toc1_1_2_)    \n",
    "- [Setup](#toc2_)    \n",
    "- [Strategies](#toc3_)    \n",
    "  - [Context extension Retriever](#toc3_1_)    \n",
    "  - [Multi-query Retriever](#toc3_2_)    \n",
    "  - [Self-query Retriever](#toc3_3_)    \n",
    "- [To go the extra mile - Combination of retrieval methods](#toc4_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc2_'></a>[Setup](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "os.chdir(Path.cwd().joinpath(\"..\"))\n",
    "print(Path.cwd())\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.query_constructor.base import AttributeInfo\n",
    "from langchain.retrievers.ensemble import EnsembleRetriever\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "from langchain.schema import Document\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "\n",
    "from lib.models import embeddings, llm\n",
    "from lib.utils import (\n",
    "    build_vector_store,\n",
    "    drop_document_duplicates,\n",
    "    get_all_documents,\n",
    "    load_documents,\n",
    "    load_vector_store,\n",
    "    split_documents_basic,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic retriever simply embeds the query using the same embedding model, and then evaluates the similarity between the embedded query and the vectorized chunks and returns the chunks with the highest similarity.\n",
    "\n",
    "The important parameters for a basic retriever are the chosen similarity measure (chroma default is L2, but can also be cosine similarity, which is commonly used) and the number of retrieved chunks, marked by K.\n",
    "However, the parameter for the similairty measure needs to be specified when building the vector store as it influences how the vectors are positionned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initiate our simple vector store based on the two pdf documents we use, using cosine similarity, which is preferred for high-dimensional embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_CHUNK_SIZE = 512\n",
    "\n",
    "# build vector_store\n",
    "base_documents = split_documents_basic(load_documents(\"data/3_docs\"), BASE_CHUNK_SIZE, include_linear_index=True)\n",
    "\n",
    "build_vector_store(\n",
    "    base_documents,\n",
    "    embeddings,\n",
    "    collection_name=\"3_docs\",\n",
    "    distance_function=\"cosine\",\n",
    "    erase_existing=True,\n",
    ")\n",
    "\n",
    "# Load Vector store / retriever\n",
    "chroma_vector_store = load_vector_store(embeddings, \"3_docs\")\n",
    "chroma_vector_store_retriever = chroma_vector_store.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_query = \"How can cloud adoption help financial institutions\"\n",
    "embedded_test_query = embeddings.embed_query(test_query)\n",
    "embedded_test_query[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chroma encapsulates all of the process in a simple search method with a text input, but for the sake of explanation we will use a more detailed approach. We will use a k=5 parameter and display the scores for each chunk.\n",
    "\n",
    "The scoring method used here is cosine distance which is calculated as 1-cosine_similarity(x,y), with cosine_similarity(x,y)=dot(x,y)/(norm(x)*norm(y))\n",
    "\n",
    "This score can take values in a [0,2] range, with smaller scores indicating more similarity.\n",
    "\n",
    "Here we extract the chunks using two methods. The first one which also retrieves the score, takes as input the query itself and does the embedding automatically using the model specified when loading/building the vector store. The second method is more low-level in that it takes in a vector corresponding to the embedded query, and returns the similar chunks. This does not provide the scores however."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_chunks_basic_wscore = chroma_vector_store.similarity_search_with_score(\n",
    "    test_query,\n",
    "    k=5,\n",
    ")\n",
    "\n",
    "retrieved_chunks_basic = chroma_vector_store.similarity_search_by_vector(\n",
    "    embedding=embedded_test_query,\n",
    "    k=5,\n",
    ")\n",
    "\n",
    "for chunk in retrieved_chunks_basic_wscore:\n",
    "    print(\n",
    "        f\"\"\"SCORE:\n",
    "        {chunk[1]}\n",
    "\n",
    "        CHUNK:\n",
    "        {chunk[0].page_content}\n",
    "        \"\"\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc3_'></a>[Strategies](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc3_1_'></a>[Context extension Retriever](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Retrieval Strategy is a good start to augmenting the context for the query answer. The basic premise is to retrieve the chunks that are linked with the similarity-fetched chunks in the document structure. Indeed, the assumption is that such chunks can also contain bits of relevant information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common and simple way to implement this is to add the adjacent chunks, i.e. for each chunk retrieved by similarity, to also retrieve the chunks corresponding to the text right before and after. For this case, we want the overlap between the chunks to be limited, as it would defeat the purpose.Here we have around 10% overlap which is fine.\n",
    "\n",
    "This is simple to implement if the order of the documents was encoded into the chunk metadata (which we did when creating our vector store):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_indexed_chunk(vector_store: Chroma, source: str, index: int) -> Document:\n",
    "    \"\"\"Extracts a chunk from the vector_store based on a linear index and a document source.\n",
    "\n",
    "    Args:\n",
    "        vector_store (Chroma): chroma vector store\n",
    "        source (str): file name\n",
    "        index (int): linear index corresponding to chunk\n",
    "    \"\"\"\n",
    "    try:\n",
    "        indexed_chunk = vector_store.get(\n",
    "            where={\n",
    "                \"$and\": [\n",
    "                    {\"source\": source},\n",
    "                    {\"linear_index\": index},\n",
    "                ]\n",
    "            }\n",
    "        )\n",
    "        return Document(\n",
    "            page_content=indexed_chunk[\"documents\"][0],\n",
    "            metadata=indexed_chunk[\"metadatas\"][0],\n",
    "        )\n",
    "    except IndexError:\n",
    "        raise IndexError\n",
    "\n",
    "\n",
    "def retrieve_adjacent_chunks(retrieved_chunks: list[Document], vector_store: Chroma) -> list[Document]:\n",
    "    \"\"\"From a list of chunks, augments list with adjacent chunks if they exist.\n",
    "\n",
    "    Args:\n",
    "        retrieved_chunks (list[Document]): list of chunks\n",
    "        vector_store (Chroma): chroma vector store\n",
    "\n",
    "    Returns:\n",
    "        list[Dcoument]: augmented list of chunks\n",
    "    \"\"\"\n",
    "    retrieved_chunks_with_adjacents = []\n",
    "    for chunk in retrieved_chunks:\n",
    "        try:\n",
    "            retrieved_chunks_with_adjacents.append(\n",
    "                extract_indexed_chunk(\n",
    "                    vector_store,\n",
    "                    source=chunk.metadata[\"source\"],\n",
    "                    index=chunk.metadata[\"linear_index\"] - 1,\n",
    "                )\n",
    "            )\n",
    "        except IndexError:\n",
    "            pass\n",
    "        retrieved_chunks_with_adjacents.append(chunk)\n",
    "        try:\n",
    "            retrieved_chunks_with_adjacents.append(\n",
    "                extract_indexed_chunk(\n",
    "                    vector_store,\n",
    "                    source=chunk.metadata[\"source\"],\n",
    "                    index=chunk.metadata[\"linear_index\"] + 1,\n",
    "                )\n",
    "            )\n",
    "        except IndexError:\n",
    "            pass\n",
    "    return drop_document_duplicates(retrieved_chunks_with_adjacents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To load the vector store without having to rebuild it, we can use the Chroma class with a persistent directory:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then retrieve the adjacent chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_chunks_list = retrieve_adjacent_chunks(retrieved_chunks_basic, chroma_vector_store)\n",
    "\n",
    "adj_chunks_list_unique = drop_document_duplicates(adj_chunks_list)\n",
    "\n",
    "print(len(adj_chunks_list))\n",
    "print(len(adj_chunks_list_unique))\n",
    "adj_chunks_list_unique[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc3_2_'></a>[Multi-query Retriever](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A more advanced and quite common retriever augmentation technique is to use what is called a multi-query retriever. In this case, the idea is to use a LLM agent to reformulate the user question in multiple different ways, and then perform the retrieval for each new query, returning all unique chunks.\n",
    "\n",
    "This allows for the retriever to minimize how much of the matching is done on the query form (syntax, vocabulary...) and instead focuses more on the semantic meaning of the query. Langchain includes this functionnality, allowing a retriever initialization from a LLM and a vector store."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set the logging, in order to display the new queries generated by the multi-query retriever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig()\n",
    "logging.getLogger(\"langchain.retrievers.multi_query\").setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mq_retriever = MultiQueryRetriever.from_llm(\n",
    "    retriever=chroma_vector_store.as_retriever(search_kwargs={\"k\": 5}),\n",
    "    llm=llm,\n",
    "    include_original=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameter for controlling the number of reformulated queries is within the prompt itself. The default Langchain prompt specifies three reformulations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see an example of retrieved documents here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mq_retriever.invoke(test_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc3_3_'></a>[Self-query Retriever](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lesser used but still interesting method is a Self Query Retriever. This technique incorporates an LLM model to do smart filtering on the metadata based on the query, if it contains some information. In our case, the sematnic information in the metadata is limited, but it can work well for more expansive metadata. We can still showcase an example where the SQ retriever will filter on the source document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sq_test_query = \"In the Data for Finance Report, how can cloud adoption help financial institutions\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this retriever to work, we need to add some context, notably specifying what the metadata and content corresponds to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_description = [\n",
    "    AttributeInfo(\n",
    "        name=\"linear_index\",\n",
    "        description=\"The order of the document within the greater corpus\",\n",
    "        type=\"integer\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"page\",\n",
    "        description=\"The page number of the document\",\n",
    "        type=\"integer\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"source\",\n",
    "        description=\"The source file from which the document was extracted\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "content_description = \"Reports on application of data-driven techniques (including AI/ML) to the financial sector, as well as convictions on data quality management\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now initialize and invoke our retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_retriever = SelfQueryRetriever.from_llm(\n",
    "    llm=llm,\n",
    "    vectorstore=chroma_vector_store,\n",
    "    metadata_field_info=metadata_description,\n",
    "    document_contents=content_description,\n",
    ")\n",
    "sf_query = \"Do a summary of the page 6\"\n",
    "results = sf_retriever.invoke(sf_query)\n",
    "for chunk in results:\n",
    "    print(f\"\\npage NÂ°{chunk.metadata['page']} : {chunk.page_content[0:100].strip()}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc4_'></a>[To go the extra mile - Combination of retrieval methods](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will showcase a custom retriever that combines multiple methods. In this case, we will integrate bm25, which is an NLP model that scores texts based on a custom similarity metric not-related to an embedding model (think of it as close to TF_IDF). We will use this in combination with a multiquery retriever to refine our retrieved chunks. Then, we will augment the retrieved chunks with their neighbors to get a refined set of chunks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of combining a non-embedding retriever (bm25) with an embedding-based one is to leverage both of their strengths. Indeed the former is good at capturing word-based similarity, while the latter captures semantic similarity. We use the langchain EnsembleRetriever class to create a combination retriever. This class integrates the Reciprocal Rank Fusion algorithm to rank the combined chunks, prioritzing those who appear in both methods.\n",
    "\n",
    "For the bm25 retriever, we cannot invoke the vector store directly, as it simply takes the list of documents pre-embedding. We have developped a function to do this from the vector database, without having to reload the raw documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_list = get_all_documents(chroma_vector_store)\n",
    "\n",
    "bm25_retriever = BM25Retriever.from_documents(document_list)\n",
    "bm25_retriever.k = 5\n",
    "\n",
    "mq_retriever = MultiQueryRetriever.from_llm(\n",
    "    retriever=chroma_vector_store.as_retriever(search_kwargs={\"k\": 5}),\n",
    "    llm=llm,\n",
    "    include_original=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create our ensemble retriever and invoke it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_retriever = EnsembleRetriever(retrievers=[bm25_retriever, mq_retriever], weights=[0.5, 0.5])\n",
    "\n",
    "combined_chunks = combined_retriever.invoke(test_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we also retrieve adjacent chunks to get the extended context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combination_chunks = retrieve_adjacent_chunks(combined_chunks, chroma_vector_store)\n",
    "\n",
    "combination_chunks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
