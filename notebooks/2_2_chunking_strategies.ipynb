{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a>[Chunking](#toc0_)\n",
    "\n",
    "In this notebook, we will explore the next part of the pipeline: Chunking. Chunking is the process of splitting the parsed documents into fragments of text (chunks) to be embedded in the vector store. Chunking optimisation consists in building the chunks that best represent one semantic brick, i.e. the chunks that will be easiest to retrieve later on in the pipeline.\n",
    "\n",
    "Here, we will showcase a basic chunking strategy, and then study three other ways to perform better chunking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [Chunking](#toc1_)    \n",
    "- [Setup](#toc2_)    \n",
    "- [Strategies](#toc3_)    \n",
    "  - [Basic text splitter](#toc3_1_)    \n",
    "  - [Hierarchical chunking](#toc3_2_)    \n",
    "  - [Semantic chunking](#toc3_3_)    \n",
    "- [To go the extra mile - Agentic chunking](#toc4_)    \n",
    "  - [Multiple techniques](#toc4_1_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc2_'></a>[Setup](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "os.chdir(Path.cwd().joinpath(\"..\"))\n",
    "print(Path.cwd())\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import HTML, display\n",
    "from langchain.schema import Document\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_text_splitters import (\n",
    "    HTMLHeaderTextSplitter,\n",
    "    RecursiveCharacterTextSplitter,\n",
    ")\n",
    "\n",
    "from lib.models import embeddings, llm\n",
    "from lib.utils import AgentChunker, load_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will perform the chunking on the following webpage that gives some info about the solar system: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For demonstration purposes, we will look at either page 42 of the Data for Finance report as an example to illustrate our different chunking strategies, or the follwing generated document:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"data/2_docs\"\n",
    "HTML_DOCUMENT_PATH = f\"{DATA_PATH}/solar_system.html\"\n",
    "html_document_title = Path(HTML_DOCUMENT_PATH).name\n",
    "PDF_DOCUMENT_PATH = f\"{DATA_PATH}/Artefact-data-for-Finance-Report.pdf\"\n",
    "pdf_document_title = Path(PDF_DOCUMENT_PATH).name\n",
    "EXAMPLE_PAGE_NUMBER = 42\n",
    "BASE_CHUNK_SIZE = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(HTML_DOCUMENT_PATH, \"r\") as file:\n",
    "    html_document = file.read()\n",
    "display(HTML(html_document))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc3_'></a>[Strategies](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc3_1_'></a>[Basic text splitter](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A widely used and simple to implement chunking strategy is to use a recursive character splitter. \n",
    "\n",
    "This text splitter is the recommended one for generic text. It is parameterized by a list of characters. It tries to split on them in order until the chunks are small enough. The default list is [\"\\n\\n\", \"\\n\", \" \", \"\"]. This has the effect of trying to keep all paragraphs (and then sentences, and then words) together as long as possible, as those would generically seem to be the strongest semantically related pieces of text. We have made sure that these characters are indeed present in the example document.\n",
    "\n",
    "For now, we will use a basic chunk overlap of 26 characters (1/10th of chunk size). For chunk size, we need to consider the embedding process later on. Indeed, as the embedding model we use is OpenAI's ada-002 which embeds into 1536 dimesnions, we will use a chunk size of 256 characters to keep the embeddings coherent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunker = RecursiveCharacterTextSplitter(chunk_size=BASE_CHUNK_SIZE, chunk_overlap=round(BASE_CHUNK_SIZE / 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the plain text of this webpage to perform our basic recursive split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_document_text = Document(\n",
    "    page_content=BeautifulSoup(html_document, \"html.parser\").get_text(separator=\"/n\"),\n",
    "    metadata={\"source\": html_document_title},\n",
    ")\n",
    "basic_chunks = chunker.split_documents([html_document_text])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We take a closer look at the chunks from this example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{len(basic_chunks)} chunks\")\n",
    "\n",
    "for i in range(len(basic_chunks)):\n",
    "    print(f\"\\n###\\nChunk number {i + 1}:\\n{basic_chunks[i].page_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clear that these chunks do not represent an optimal strategy. We will now explore more thorough ways of doing such chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc3_2_'></a>[Hierarchical chunking](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this to work, we need such structure to be encoded directly into the documents, which is not always the case for raw text inputs. It thus falls onto the parsing step's responsibility to  ensure this structure is stored. Some formats, such as Html or Markdown, include this structure automatically, and thus are a good starting point with dedicated chunkers in Langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Html is particularly interesting because it allows to easily chunk data scrapped from web pageselements. There also is an integrated HTML splitter in Langchain\n",
    "\n",
    "We specify the headers on which we want to split in the config file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now use the built-in HTML splitter from Langchain, and then go through our basic recursive splitter in order to keep the desired chunks size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPLITTING_HEADERS = [\n",
    "    (\"h1\", \"Header 1\"),\n",
    "    (\"h2\", \"Header 2\"),\n",
    "    (\"h3\", \"Header 3\"),\n",
    "    (\"h4\", \"Header 4\"),\n",
    "]\n",
    "\n",
    "hierarchical_chunks = HTMLHeaderTextSplitter(headers_to_split_on=SPLITTING_HEADERS).split_text(html_document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want fixed size chunks (for embedding purposes), we can then split the larger chunks even more using the typical Recursive text splitter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hierarchical_chunks = chunker.split_documents(hierarchical_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We take a closer look at the chunks from this example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{len(hierarchical_chunks)} chunks\")\n",
    "\n",
    "for i, chunk in enumerate(hierarchical_chunks, start=1):\n",
    "    print(f\"\\n###\\nChunk number {i}:\\n{chunk.page_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also see that the structure of the document is integrated within the chunk metadata, which can be very useful when optimizing the retrieval strategy. Notably, the titles of all the headers in the hierarchy above the chunk are preserved, which can come in very handy in the retrieval phase to retrieve other relevant chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"We have {len(hierarchical_chunks)} chunks\")\n",
    "hierarchical_chunks[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These chunks are already much better, and can be used for a basic RAG pipeline. However, we will look at even more advanced ways to do this chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc3_3_'></a>[Semantic chunking](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next approach is to use Semantic Chunking. This relatively new method uses embeddings to determine semantic breaking points within the text.\n",
    "\n",
    "This method embeds sentences grouped together through a rolling window (default is three sentences) and then calculates embedding distances between adjacent groups of sentences. For example if we have six sentences:\n",
    "* The groups of sentences to be embedded are: [1], [1,2], [1,2,3], [2,3,4], [3,4,5], [4,5,6], [5,6], [6]\n",
    "* The calculated distances are: d([1],[2,3,4]), d([1,2],[3,4,5]), d([1,2,3],[4,5,6]), d([2,3,4],[5,6]), d([3,4,5],[6])\n",
    "\n",
    "It then looks at all calculated embedding distances and splits the documents along the \"sentence boundaries\" where the distance is above a certain threshold.\n",
    "* Percentile: distance greater than Xth percentile\n",
    "* Standard Deviation: distance above X standard deviations\n",
    "* Interquartile: distance outside of the quartiles\n",
    "* Gradient: for specific documents with high levels of similarity, looks for gradient anomalies in distances instead of a fixed value\n",
    "\n",
    "For this example, we will use page 42 of the Data for Finance Report document, as well as the ada-002 embedding model deployed on Azure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page42 = [\n",
    "    doc\n",
    "    for doc in load_documents(DATA_PATH)\n",
    "    if (doc.metadata[\"source\"] == PDF_DOCUMENT_PATH and doc.metadata[\"page\"] == EXAMPLE_PAGE_NUMBER - 1)\n",
    "]\n",
    "\n",
    "print(page42[0].metadata)\n",
    "print(page42[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_chunks = SemanticChunker(\n",
    "    embeddings,\n",
    "    breakpoint_threshold_type=\"percentile\",\n",
    "    buffer_size=1,\n",
    "    breakpoint_threshold_amount=70,\n",
    ").split_documents(page42)\n",
    "\n",
    "print(f\"{len(semantic_chunks)} chunks\")\n",
    "\n",
    "for i in range(len(semantic_chunks)):\n",
    "    print(\n",
    "        f\"\"\"Chunk {i + 1}:\n",
    "          {semantic_chunks[i].page_content}\n",
    "          \"\"\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have set a rather arbitrary 70% threshold, i.e. we split at boundary points where the embedding distance between the two adjacent sentence groups is in the top 30% of distances. This gives us 6 chunks with no fixed size, which seem to make some sense semantically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc4_'></a>[To go the extra mile - Agentic chunking](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the last technique we will use is Agentic Chunking. Agentic Chunking is more of a general concept than a defined technique, and consists in directly using a LLM to identify the semantic chunks within the text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are multiple ways of using an Agent to define chunks. Here we will explore two ways that mirror the previously explored methods:\n",
    "* Recursive Agent-based chunking: we use the agent to split a text in two semantically distinct parts, and do so recursively until we have attained critical size\n",
    "* Iterative Agent-based chunking: we iteratively go through all sentences in the document. At each point, we ask the agent if the sentence is semantically part of a new chunk or if it should be merged withe the previous chunk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initiate the agent we will use, in this case GPT-4o. As these techniques are still experimental we do not have an integrated Langchain method, but we have developed simple functins for both techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chunker_recursive = AgentChunker(agent=llm, chunk_size=BASE_CHUNK_SIZE)\n",
    "\n",
    "agent_chunks_recursive = llm_chunker_recursive.split_documents(page42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(agent_chunks_recursive)):\n",
    "    print(\n",
    "        f\"\"\"Chunk {i + 1}:\n",
    "          {agent_chunks_recursive[i].page_content}\n",
    "          \"\"\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that these chunks have good semantical separation, and thus this method performs well, even though it is also very costly. We also try the other method, which is iterative chunking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chunker_iterative = AgentChunker(\n",
    "    agent=llm,\n",
    "    chunk_size=BASE_CHUNK_SIZE,\n",
    "    recursive=False,\n",
    ")\n",
    "\n",
    "agent_chunks_iterative = llm_chunker_iterative.split_documents(page42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(agent_chunks_iterative)):\n",
    "    print(\n",
    "        f\"\"\"Chunk {i + 1}:\n",
    "          {agent_chunks_iterative[i].page_content}\n",
    "          \"\"\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method also gives good chunks, albeit with no chunk length harmony."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc4_1_'></a>[Multiple techniques](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will combine the three last techniques in order to to showcase a thorough chunking process. We will use them in the following order:\n",
    "* We will first perform hierarchical chunking to split along document structure elements and keep said structure in the metadata\n",
    "* Within these bricks, we will perform semantic chunking based on the 75th percentile to refine our chunks using our embeddings\n",
    "* Finally, if some chunks are still too large, we will use recursive Agent-based chunking to split them until we recah the desired character length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunker_1 = HTMLHeaderTextSplitter(headers_to_split_on=SPLITTING_HEADERS)\n",
    "\n",
    "chunker_2 = SemanticChunker(\n",
    "    embeddings,\n",
    "    breakpoint_threshold_type=\"percentile\",\n",
    "    buffer_size=1,\n",
    "    breakpoint_threshold_amount=70,\n",
    ")\n",
    "\n",
    "chunker_3 = AgentChunker(llm, chunk_size=BASE_CHUNK_SIZE, recursive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We do not have parsed document yet so use the generated solar system page\n",
    "\n",
    "full_pipeline_chunks = chunker_3.split_documents(chunker_2.split_documents(chunker_1.split_text(html_document)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(full_pipeline_chunks)):\n",
    "    print(\n",
    "        f\"\"\"Chunk {i + 1}:\n",
    "          {full_pipeline_chunks[i].page_content}\n",
    "          \"\"\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
