{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a>[Guardrails Strategies](#toc0_)\n",
    "\n",
    "In this notebook, we showcase different guardrails strategies that can be used separately or combined to ensure the generated answers are not hallucinated or harmfull. We will: \n",
    "- Ensure the generated answer is grounded in the context retrieved, minimizing hallucinations using: \n",
    "  - Custom methods\n",
    "  - Ragas Framework\n",
    "- See other gardrails strategies using Giskard scanner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [Set Up](#toc2_)    \n",
    "- [1) Custom method: Verify that the answer is grounded in the context retrieved](#toc3_)    \n",
    "- [2) Ragas framework: Faithfullness & other metrics](#toc4_)    \n",
    "- [3) Giskard: a testing framework for LLM applications.](#toc5_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc2_'></a>[Set Up](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "os.chdir(Path.cwd().joinpath(\"..\"))\n",
    "print(Path.cwd())\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "from datasets import Dataset\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema import Document, StrOutputParser\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain_core.runnables import RunnableLambda, RunnableParallel\n",
    "from ragas import evaluate\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.metrics import (\n",
    "    Faithfulness,\n",
    "    ResponseRelevancy,\n",
    ")\n",
    "\n",
    "from lib.models import embeddings, llm\n",
    "from lib.utils import (\n",
    "    build_vector_store,\n",
    "    load_documents,\n",
    "    load_vector_store,\n",
    "    split_documents_basic,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_CHUNK_SIZE = 1024\n",
    "\n",
    "# build vector_store\n",
    "base_documents = split_documents_basic(load_documents(\"data/5_docs\"), BASE_CHUNK_SIZE, include_linear_index=True)\n",
    "\n",
    "build_vector_store(\n",
    "    base_documents,\n",
    "    embeddings,\n",
    "    collection_name=\"5_docs\",\n",
    "    distance_function=\"cosine\",\n",
    "    erase_existing=False,\n",
    ")\n",
    "\n",
    "# Load Vector store / retriever\n",
    "chroma_vector_store = load_vector_store(embeddings, \"5_docs\")\n",
    "chroma_vector_store_retriever = chroma_vector_store.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "    \"According to the IPCC report, what are key risks in the Europe?\",\n",
    "    \"Is sea level rise avoidable?\",\n",
    "    \"Will sea level rise stop?\",\n",
    "    \"What are the main climate risks in North America?\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To illustrate the use of the 2 guardrail strategies showcased in this notebook, we will create a simple Retrieval-Augmented Generation (RAG) pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_TEMPLATE = \"\"\"You are the Climate Assistant, a helpful AI assistant.\n",
    "Your task is to answer common questions on climate change.\n",
    "You will be given a question and relevant excerpts from the IPCC Climate Change Synthesis Report (2023).\n",
    "Please provide short and clear answers based on the provided context. Be polite and helpful.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Your answer:\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def _format_docs(docs: list[Document]) -> list[str]:  # noqa: UP006\n",
    "    return [doc.page_content for doc in docs]\n",
    "\n",
    "\n",
    "def _concate_chunk(chunks: list[str]) -> str:  # noqa: UP006\n",
    "    return \"\\n\\n\".join(chunk for chunk in chunks)\n",
    "\n",
    "\n",
    "response_prompt = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)\n",
    "\n",
    "rag_chain = (\n",
    "    RunnableParallel(\n",
    "        {\n",
    "            \"source_documents\": chroma_vector_store_retriever,\n",
    "            \"question\": RunnablePassthrough(),\n",
    "        }\n",
    "    )\n",
    "    .assign(chunks=RunnableLambda(itemgetter(\"source_documents\")) | _format_docs)\n",
    "    .assign(context=RunnableLambda(itemgetter(\"chunks\")) | _concate_chunk)\n",
    "    .assign(answer=response_prompt | llm | StrOutputParser())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = rag_chain.batch(questions)\n",
    "print(responses[0][\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc3_'></a>[1) Custom method: Verify that the answer is grounded in the context retrieved](#toc0_)\n",
    "\n",
    "In this section, we implemented the function `check_grounded_in_context` that determines if the response is grounded in the retrieved context. The function returns 'GROUNDED' if the response is fully supported by the context, and 'NOT_GROUNDED' otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure response is grounded in context\n",
    "def check_grounded_in_context(response: str, context: list[Document]) -> bool:\n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "        \"\"\"Given the following context and response, determine if the response is fully grounded in the context. \\\n",
    "If it is, return 'GROUNDED'. If it contains information not present in the context, return 'NOT_GROUNDED'.\\n\\n\n",
    "Context: {context}\\n\n",
    "Response: {response}\\n\n",
    "Determination:\"\"\"\n",
    "    )\n",
    "    chain = prompt | llm | StrOutputParser()\n",
    "    result = chain.invoke({\"context\": context, \"response\": response})\n",
    "    return result.strip() == \"GROUNDED\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for indx, response in enumerate(responses):\n",
    "    is_grounded = check_grounded_in_context(response[\"answer\"], response[\"context\"])\n",
    "    print(f\"question #{indx} : {is_grounded}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc4_'></a>[2) Ragas framework: Faithfullness & other metrics](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "source": [
    "Ragas is an open-source evaluation framework for Retrieval-Augmented Generation (RAG) systems, providing standardized metrics to measure answer quality, relevance, and faithfulness. It helps developers benchmark and improve RAG pipelines with reproducible and comparable results. \n",
    "\n",
    "We will use 2 metrics:\n",
    "\n",
    "1. [`Faithfulness`](https://docs.ragas.io/en/stable/concepts/metrics/available_metrics/faithfulness/#faithfulness) - Measures the factual consistency of the answer to the context based on the question. A low score indicates that the answer contains hallucinations (information not supported by the context), while a high score means the response is faithful to the source.\n",
    "2. [`ResponseRelevancy`](https://docs.ragas.io/en/stable/concepts/metrics/available_metrics/answer_relevance/#faithfullness-with-hhem-21-open) - Measures how relevant the answer is to the question.\n",
    "the question.\n",
    "\n",
    "To explore other metrics, check the [metrics guide](https://docs.ragas.io/en/stable/concepts/metrics/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap llm & embeddings for Ragas\n",
    "ragas_llm = LangchainLLMWrapper(llm)\n",
    "ragas_embeddings = LangchainEmbeddingsWrapper(embeddings)\n",
    "\n",
    "# Build dataset for Ragas\n",
    "data = {\n",
    "    \"question\": questions,\n",
    "    \"answer\": [out[\"answer\"] for out in responses],\n",
    "    \"contexts\": [out[\"chunks\"] for out in responses],\n",
    "}\n",
    "\n",
    "dataset = Dataset.from_dict(data)\n",
    "\n",
    "metrics = [\n",
    "    Faithfulness(llm=ragas_llm),\n",
    "    ResponseRelevancy(llm=ragas_llm, embeddings=ragas_embeddings),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ragas_results = evaluate(dataset=dataset, metrics=metrics, llm=ragas_llm, embeddings=ragas_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ragas_results.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc5_'></a>[3) Giskard: a testing framework for LLM applications.](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Giskard is an open-source framework that helps data scientists and machine learning teams automatically test, debug, and monitor their models for biases, errors, and vulnerabilities. It enables safe and reliable deployment of AI systems by providing collaborative tools to evaluate model quality and trustworthiness.\n",
    "\n",
    "Giskard is a broad AI testing framework for detecting biases and vulnerabilities in any ML model, while Ragas focuses specifically on evaluating RAG pipelines for LLMs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import giskard\n",
    "import pandas as pd\n",
    "\n",
    "giskard.llm.set_llm_model(\n",
    "    f\"azure/{os.getenv('LLM_AZURE_OPENAI_DEPLOYMENT_NAME')}\",\n",
    "    api_base=os.getenv(\"LLM_AZURE_OPENAI_ENDPOINT\"),\n",
    "    api_version=os.getenv(\"LLM_AZURE_OPENAI_API_VERSION\"),\n",
    "    api_key=os.getenv(\"LLM_AZURE_OPENAI_API_KEY\"),\n",
    ")\n",
    "\n",
    "giskard.llm.set_embedding_model(\n",
    "    f\"azure/{os.getenv('LLM_AZURE_OPENAI_DEPLOYMENT_NAME')}\",\n",
    "    api_base=os.getenv(\"EMBEDDINGS_AZURE_OPENAI_ENDPOINT\"),\n",
    "    api_version=os.getenv(\"EMBEDDINGS_AZURE_OPENAI_API_VERSION\"),\n",
    "    api_key=os.getenv(\"EMBEDDINGS_AZURE_OPENAI_API_KEY\"),\n",
    ")\n",
    "\n",
    "\n",
    "def model_predict(df: pd.DataFrame) -> list:\n",
    "    \"\"\"Wraps the LLM call in a simple Python function.\n",
    "\n",
    "    The function takes a pandas.DataFrame containing the input variables needed\n",
    "    by your model, and must return a list of the outputs (one for each row).\n",
    "    \"\"\"\n",
    "    return [rag_chain.invoke(question)[\"answer\"] for question in df[\"question\"]]\n",
    "\n",
    "\n",
    "# Don’t forget to fill the `name` and `description`: they are used by Giskard\n",
    "# to generate domain-specific tests.\n",
    "giskard_model = giskard.Model(\n",
    "    model=model_predict,\n",
    "    model_type=\"text_generation\",\n",
    "    name=\"Climate Change Question Answering\",\n",
    "    description=\"This model answers any question about climate change based on IPCC reports\",\n",
    "    feature_names=[\"question\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "giskard_dataset = giskard.Dataset(pd.DataFrame({\"question\": questions}), target=None)\n",
    "\n",
    "print(giskard_model.predict(giskard_dataset).prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for hallucination\n",
    "report = giskard.scan(giskard_model, giskard_dataset, only=\"hallucination\")\n",
    "\n",
    "# for the full report, run (can take several minutes)\n",
    "# report = giskard.scan(giskard_model, giskard_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: The hallucination check from the Ragas is more a Sycophancy Detector. See [Giskard's documentation](https://docs.giskard.ai/en/stable/knowledge/llm_vulnerabilities/index.html#llm-assisted-detectors) for more details. \n",
    "\n",
    "> Sycophancy detector\n",
    "> The sycophancy detector (see :class:~giskard.scanner.llm.LLMBasicSycophancyDetector) is an example of an LLM-assisted detector. Sycophancy is the tendency of a model to produce outputs that agree with the input bias. This is often linked to model hallucination, and allows us to test for model coherency and hallucination even when we don’t have access to specific ground truth data to verify the model outputs.  \n",
    "> To detect sycophantic behavior, we will use an LLM to generate pairs of adversarial inputs tailored for the model under test. Each pair will contain queries that are biased in opposite ways, but which should produce the same answer from the model.  \n",
    "> As an example, consider a question-answering model on climate change based on reporting by the IPCC (Intergovernmental Panel on Climate Change). Our LLM-assisted input generation will generate pairs of questions, at least one of which will have a specific bias or make assumptions that contradict the other.\"\n",
    "\n",
    "\n",
    "When we use the Giscard scan, we only use the question and the output generated and an other LLM is the judge.\n",
    "It's also possible to have custom metric or to embedd ragas metrics in Giskard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report.to_html(\"output/5_giskard_report.html\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
